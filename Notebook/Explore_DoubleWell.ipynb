{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8nWptLxRTXC"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A6iMkl60RSir"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import torch \n",
    "from torch import distributions\n",
    "from torch import nn\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9XhRqMiRXHY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytY-ZnUJRYE6"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7x23wIRQGxQ0"
   },
   "source": [
    "### DoubleWell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "g40uuHv0RaTq"
   },
   "outputs": [],
   "source": [
    "class DoubleWell(object):\n",
    "\n",
    "    params_default = {'a4' : 1.0,\n",
    "                      'a2' : 6.0,\n",
    "                      'a1' : 1.0,\n",
    "                      'k' : 1.0,\n",
    "                      'dim' : 2}\n",
    "\n",
    "    def __init__(self, params=None):\n",
    "        # set parameters\n",
    "        if params is None:\n",
    "            params = self.__class__.params_default\n",
    "        self.params = params\n",
    "\n",
    "        # useful variables\n",
    "        self.dim = self.params['dim']\n",
    "\n",
    "    def energy(self, x):\n",
    "        dimer_energy =self.params['a4'] * x[:, 0] ** 4 - self.params['a2'] * x[:, 0] ** 2 + self.params['a1'] * x[:, 0]\n",
    "        oscillator_energy = 0.0\n",
    "        if self.dim == 2:\n",
    "            oscillator_energy = (self.params['k'] / 2.0) * x[:, 1] ** 2\n",
    "        if self.dim > 2:\n",
    "            oscillator_energy = np.sum((self.params['k'] / 2.0) * x[:, 1:] ** 2, axis=1)\n",
    "        return  dimer_energy + oscillator_energy\n",
    "\n",
    "#    def energy_tf(self, x):\n",
    "#        return self.energy(x)\n",
    "\n",
    "    def plot_dimer_energy(self, axis=None, temperature=1.0):\n",
    "        \"\"\" Plots the dimer energy to the standard figure \"\"\"\n",
    "        x_grid = np.linspace(-3, 3, num=200)\n",
    "        if self.dim == 1:\n",
    "            X = x_grid[:, None]\n",
    "        else:\n",
    "            X = np.hstack([x_grid[:, None], np.zeros((x_grid.size, self.dim - 1))])\n",
    "        energies = self.energy(X) / temperature\n",
    "\n",
    "#        import matplotlib.pyplot as plt\n",
    "        if axis is None:\n",
    "            axis = plt.gca()\n",
    "        #plt.figure(figsize=(5, 4))\n",
    "        axis.plot(x_grid, energies, linewidth=3, color='black')\n",
    "        axis.set_xlabel('x / a.u.')\n",
    "        axis.set_ylabel('Energy / kT')\n",
    "        axis.set_ylim(energies.min() - 2.0, energies[int(energies.size / 2)] + 2.0)\n",
    "#        plt.show()\n",
    "\n",
    "        return x_grid, energies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fN0G6XiqGe09"
   },
   "source": [
    "### BG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uUTtbtud9EPx"
   },
   "outputs": [],
   "source": [
    "class BG_RealNVP(nn.Module):\n",
    "  def __init__(self, n_hidden=256, n_block=5, target=None, prior=None, masks=None):\n",
    "    super(BG_RealNVP,self).__init__()\n",
    "\n",
    "    if target==None:\n",
    "      target=DoubleWell()\n",
    "    self.target=target\n",
    "    self.n_hidden=n_hidden\n",
    "    self.n_block=n_block\n",
    "\n",
    "    nets = lambda: nn.Sequential(nn.Linear(2, self.n_hidden), nn.ReLU(), nn.Linear(self.n_hidden, self.n_hidden), nn.ReLU(), nn.Linear(self.n_hidden, 2), nn.Tanh())\n",
    "\n",
    "    nett = lambda: nn.Sequential(nn.Linear(2, self.n_hidden), nn.ReLU(), nn.Linear(self.n_hidden, self.n_hidden), nn.ReLU(), nn.Linear(self.n_hidden, 2))\n",
    "\n",
    "    if prior==None:\n",
    "      self.prior = distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "\n",
    "    if masks==None:\n",
    "      self.masks = nn.Parameter(torch.from_numpy(np.array([[0, 1], [1, 0]] * self.n_block).astype(np.float32)), requires_grad=False)\n",
    "\n",
    "    self.nett = torch.nn.ModuleList([nett() for _ in range(len(self.masks))]) # translation\n",
    "    self.nets = torch.nn.ModuleList([nets() for _ in range(len(self.masks))]) # scaling \n",
    "\n",
    "\n",
    "  def target_energy(self,x):\n",
    "    return self.target.energy(x)\n",
    "\n",
    "  def prior_energy(self, z):\n",
    "    return 0.5*torch.linalg.norm(z,dim=1)**2\n",
    "\n",
    "  def forward_flow(self,z):\n",
    "    log_R_zx, x = z.new_zeros(z.shape[0]), z\n",
    "\n",
    "    for i in range(len(self.masks)):\n",
    "      x1= x*self.masks[i]\n",
    "\n",
    "      s=self.nets[i](x1)*(1-self.masks[i])\n",
    "      t=self.nett[i](x1)*(1-self.masks[i])\n",
    "\n",
    "      x=x1+(1-self.masks[i])* (x*torch.exp(s)+t)\n",
    "      log_R_zx+=torch.sum(s,-1)\n",
    "\n",
    "    return x, log_R_zx\n",
    "\n",
    "  def backward_flow(self,x):\n",
    "\n",
    "    log_R_xz, z = x.new_zeros(x.shape[0]), x\n",
    "\n",
    "    for i in reversed(range(len(self.masks))):\n",
    "      z1= z*self.masks[i]\n",
    "\n",
    "      s=self.nets[i](z1)*(1-self.masks[i])\n",
    "      t=self.nett[i](z1)*(1-self.masks[i])\n",
    "\n",
    "      z=z1+(1-self.masks[i])*(z-t)*torch.exp(-s)\n",
    "      log_R_xz-=torch.sum(s,-1)\n",
    "\n",
    "    return z, log_R_xz\n",
    "\n",
    "  def sample(self, batchSize):\n",
    "      z = self.prior.sample((batchSize,))\n",
    "      x, log_R_zx = self.forward_flow(z)\n",
    "      d_energy=self.prior_energy(z)-self.target_energy(x)+log_R_zx\n",
    "      reweighting = torch.exp(d_energy)\n",
    "      return z.detach().numpy(), x.detach().numpy(), reweighting.detach().numpy()\n",
    "\n",
    "  def loss(self, batch, w_ml = 1.0, w_kl = 0.0):\n",
    "      return w_ml*self.loss_ml(batch) + w_kl*self.loss_kl(batch)  \n",
    "\n",
    "  def loss_ml(self, batch_x):\n",
    "      z, log_R_xz = self.backward_flow(batch_x)\n",
    "      energy=0.5*torch.linalg.norm(z,dim=1)**2\n",
    "      return torch.mean(energy-log_R_xz)\n",
    "\n",
    "  def loss_kl(self, batch_z):\n",
    "      x, log_R_zx = self.forward_flow(batch_z)\n",
    "      energy=self.target_energy(x)\n",
    "      e_high=1e10\n",
    "      for i in range(len(energy)):\n",
    "        if abs(energy[i]) == float('inf'):\n",
    "                print(\"energy overflow detected\")\n",
    "        elif energy[i] > e_high:\n",
    "                energy[i] = e_high + torch.log(energy[i] - e_high + 1.0)\n",
    "      return torch.mean(energy - log_R_zx)  \n",
    "\n",
    "  def train_ML(self, x):\n",
    "    iter=100\n",
    "    lr=1e-3\n",
    "    batch_size=1024\n",
    "\n",
    "    training_data=x\n",
    "#    np.random.shuffle(training_data)\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=lr) \n",
    "#    training_data = training_data.astype('float32')\n",
    "    trainloader = data.DataLoader(dataset=training_data, batch_size=batch_size)\n",
    "\n",
    "    losses = []\n",
    "    t = 0 # iteration count\n",
    "    while t < iter:\n",
    "      for batch in trainloader:  \n",
    "\n",
    "        # Custom ML loss function\n",
    "        loss = self.loss_ml(batch) \n",
    "        losses.append(loss.item()) # save values for plotting later \n",
    "\n",
    "        # Training\n",
    "        optimizer.zero_grad() # Set grads to zero, else PyTorch will accumulate gradients on each backprop\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        t=t+1\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYls5PehkjrF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D885HuFSGhP1"
   },
   "source": [
    "### Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zvMI8J_kkjhy"
   },
   "outputs": [],
   "source": [
    "class Configuration(object):\n",
    "  def __init__(self, bg, X0, capacity, batch_size=1024):\n",
    "    self.bg=bg\n",
    "    self.lr=1e-3\n",
    "    self.batch_size=batch_size\n",
    "\n",
    "    self.I = np.arange(capacity)\n",
    "    I_X0=np.arange(X0.shape[0])\n",
    "    I_select=np.random.choice(I_X0, size=capacity, replace=True)\n",
    "    self.X=X0[I_select]\n",
    "\n",
    "    self.loss_train = []\n",
    "    self.acceptance_rate = []\n",
    "    self.stepsize = []\n",
    "\n",
    "  def configure(self, epochs, stepsize=0.1):\n",
    "    if stepsize is None:  # initialize stepsize when called for the first time\n",
    "        if len(self.stepsize) == 0:\n",
    "            self.stepsize.append(0.1)\n",
    "    else:\n",
    "        self.stepsize = [stepsize]\n",
    "\n",
    "    for e in range(epochs):\n",
    "      \n",
    "        #sample batch\n",
    "        I_select=np.random.choice(self.I,size=self.batch_size, replace=True)\n",
    "        x_batch=torch.from_numpy(self.X[I_select])\n",
    "\n",
    "        #train \n",
    "        loss=self.bg.train_ML(x_batch)\n",
    "        print('iter %s:' % e, 'loss = %.3f' % loss[-1])       \n",
    "\n",
    "        \n",
    "        z_batch, Jxz_batch = self.bg.backward_flow(x_batch)\n",
    "        \n",
    "        \n",
    "\n",
    "        E0 = self.bg.target_energy(x_batch) + Jxz_batch\n",
    "#        E0 = self.bg.target_energy(x_batch)\n",
    "\n",
    "\n",
    "        #methopolis\n",
    "        z_batch_new = z_batch + self.stepsize[-1] * torch.randn(z_batch.shape[0], z_batch.shape[1])\n",
    "\n",
    "        x_batch_new, Jzx_batch_new = self.bg.forward_flow(z_batch_new)\n",
    "        E1 = self.bg.target_energy(x_batch) - Jzx_batch_new\n",
    "#        E1 = self.bg.target_energy(x_batch)\n",
    "    \n",
    "    \n",
    "        #accept and replace\n",
    "        rand = -torch.log(torch.rand(self.batch_size))\n",
    "        Iacc = rand >= E1-E0\n",
    "\n",
    "        x_acc = x_batch_new[Iacc]\n",
    "        self.X[I_select[Iacc]] = x_acc.detach().numpy()\n",
    "\n",
    "        pacc = float(np.count_nonzero(Iacc)) / float(self.batch_size)\n",
    "        self.acceptance_rate.append(pacc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHNE1mFFGkYV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDIicm8avuQo"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "eV5Z59TrvwkB",
    "outputId": "17a1a4c2-c183-4f9a-8870-ffd5046a221e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = DoubleWell.params_default.copy()\n",
    "params['dim'] = 2\n",
    "double_well = DoubleWell(params=params)\n",
    "plt.figure(figsize=(5,5))\n",
    "double_well.plot_dimer_energy();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaFt0gsERa7Z"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3ip5C4IYRfnS"
   },
   "outputs": [],
   "source": [
    "BG=BG_RealNVP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "O8E6Itx1fy9Z"
   },
   "outputs": [],
   "source": [
    "X0=np.array([[-1.8,0.]]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eLCDAsIiHK-R"
   },
   "outputs": [],
   "source": [
    "Explore_DoubleWell=Configuration(BG,X0,capacity=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zASsr67R7MX",
    "outputId": "2f920ba7-2d09-4a65-e78e-b4db034c3e42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Explore_DoubleWell.stepsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oKVFq2PlHnXh",
    "outputId": "89881134-efb4-4651-d5c7-d5e51e7078ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss = 2.888\n",
      "iter 1: loss = -2.619\n",
      "iter 2: loss = -3.180\n",
      "iter 3: loss = -1.069\n",
      "iter 4: loss = -6.194\n",
      "iter 5: loss = -4.338\n",
      "iter 6: loss = -3.829\n",
      "iter 7: loss = -6.133\n",
      "iter 8: loss = -5.514\n",
      "iter 9: loss = -6.051\n",
      "iter 10: loss = -3.552\n",
      "iter 11: loss = -5.706\n",
      "iter 12: loss = -6.125\n",
      "iter 13: loss = -6.911\n",
      "iter 14: loss = -7.217\n",
      "iter 15: loss = -7.341\n",
      "iter 16: loss = -6.777\n",
      "iter 17: loss = -6.684\n",
      "iter 18: loss = -6.292\n",
      "iter 19: loss = -6.448\n",
      "iter 20: loss = -5.597\n",
      "iter 21: loss = -6.114\n",
      "iter 22: loss = -6.409\n",
      "iter 23: loss = -6.828\n",
      "iter 24: loss = -7.488\n",
      "iter 25: loss = -6.954\n",
      "iter 26: loss = -7.154\n",
      "iter 27: loss = -7.336\n",
      "iter 28: loss = -7.238\n",
      "iter 29: loss = -7.318\n",
      "iter 30: loss = -7.639\n",
      "iter 31: loss = -7.347\n",
      "iter 32: loss = -7.571\n",
      "iter 33: loss = -7.328\n",
      "iter 34: loss = -7.402\n",
      "iter 35: loss = -7.623\n",
      "iter 36: loss = -4.553\n",
      "iter 37: loss = -7.226\n",
      "iter 38: loss = -6.982\n",
      "iter 39: loss = -6.974\n",
      "iter 40: loss = -5.672\n",
      "iter 41: loss = -5.982\n",
      "iter 42: loss = -6.102\n",
      "iter 43: loss = -6.403\n",
      "iter 44: loss = -6.510\n",
      "iter 45: loss = -6.625\n",
      "iter 46: loss = -6.867\n",
      "iter 47: loss = -6.776\n",
      "iter 48: loss = -7.299\n",
      "iter 49: loss = -7.158\n",
      "iter 50: loss = -6.874\n",
      "iter 51: loss = -7.110\n",
      "iter 52: loss = -6.888\n",
      "iter 53: loss = -7.140\n",
      "iter 54: loss = -7.101\n",
      "iter 55: loss = -6.902\n",
      "iter 56: loss = -7.090\n",
      "iter 57: loss = -6.928\n",
      "iter 58: loss = -6.822\n",
      "iter 59: loss = -6.954\n",
      "iter 60: loss = -7.166\n",
      "iter 61: loss = -6.822\n",
      "iter 62: loss = -6.893\n",
      "iter 63: loss = -6.932\n",
      "iter 64: loss = -7.005\n",
      "iter 65: loss = -7.004\n",
      "iter 66: loss = -6.991\n",
      "iter 67: loss = -6.786\n",
      "iter 68: loss = -6.780\n",
      "iter 69: loss = -6.681\n",
      "iter 70: loss = -6.848\n",
      "iter 71: loss = -6.699\n",
      "iter 72: loss = -6.724\n",
      "iter 73: loss = -6.856\n",
      "iter 74: loss = -7.079\n",
      "iter 75: loss = -6.909\n",
      "iter 76: loss = -6.621\n",
      "iter 77: loss = -6.695\n",
      "iter 78: loss = -6.589\n",
      "iter 79: loss = -6.446\n",
      "iter 80: loss = -6.357\n",
      "iter 81: loss = -6.795\n",
      "iter 82: loss = -6.699\n",
      "iter 83: loss = -6.646\n",
      "iter 84: loss = -6.558\n",
      "iter 85: loss = -6.684\n",
      "iter 86: loss = -6.645\n",
      "iter 87: loss = -6.596\n",
      "iter 88: loss = -6.261\n",
      "iter 89: loss = -6.585\n",
      "iter 90: loss = -6.398\n",
      "iter 91: loss = -6.536\n",
      "iter 92: loss = -6.502\n",
      "iter 93: loss = -6.588\n",
      "iter 94: loss = -6.619\n",
      "iter 95: loss = -6.631\n",
      "iter 96: loss = -6.304\n",
      "iter 97: loss = -6.658\n",
      "iter 98: loss = -6.333\n",
      "iter 99: loss = -6.490\n",
      "iter 100: loss = -6.467\n",
      "iter 101: loss = -6.614\n",
      "iter 102: loss = -6.407\n",
      "iter 103: loss = -6.410\n",
      "iter 104: loss = -6.327\n",
      "iter 105: loss = -6.291\n",
      "iter 106: loss = -6.420\n",
      "iter 107: loss = -6.367\n",
      "iter 108: loss = -6.388\n",
      "iter 109: loss = -6.220\n",
      "iter 110: loss = -6.271\n",
      "iter 111: loss = -6.616\n",
      "iter 112: loss = -6.238\n",
      "iter 113: loss = -6.204\n",
      "iter 114: loss = -6.340\n",
      "iter 115: loss = -6.312\n",
      "iter 116: loss = -6.210\n",
      "iter 117: loss = -5.999\n",
      "iter 118: loss = -6.141\n",
      "iter 119: loss = -6.347\n",
      "iter 120: loss = -5.693\n",
      "iter 121: loss = -6.081\n",
      "iter 122: loss = -5.822\n",
      "iter 123: loss = -5.949\n",
      "iter 124: loss = -6.133\n",
      "iter 125: loss = -5.954\n",
      "iter 126: loss = -6.023\n",
      "iter 127: loss = -6.110\n",
      "iter 128: loss = -5.900\n",
      "iter 129: loss = -6.224\n",
      "iter 130: loss = -6.067\n",
      "iter 131: loss = -5.795\n",
      "iter 132: loss = -5.991\n",
      "iter 133: loss = -5.902\n",
      "iter 134: loss = -5.875\n",
      "iter 135: loss = -5.734\n",
      "iter 136: loss = -5.834\n",
      "iter 137: loss = -6.091\n",
      "iter 138: loss = -5.814\n",
      "iter 139: loss = -5.831\n",
      "iter 140: loss = -5.853\n",
      "iter 141: loss = -5.773\n",
      "iter 142: loss = -5.889\n",
      "iter 143: loss = -5.896\n",
      "iter 144: loss = -5.905\n",
      "iter 145: loss = -5.826\n",
      "iter 146: loss = -5.443\n",
      "iter 147: loss = -5.630\n",
      "iter 148: loss = -5.573\n",
      "iter 149: loss = -5.616\n",
      "iter 150: loss = -5.790\n",
      "iter 151: loss = -5.678\n",
      "iter 152: loss = -5.408\n",
      "iter 153: loss = -5.692\n",
      "iter 154: loss = -5.572\n",
      "iter 155: loss = -5.567\n",
      "iter 156: loss = -5.676\n",
      "iter 157: loss = -5.442\n",
      "iter 158: loss = -5.560\n",
      "iter 159: loss = -5.576\n",
      "iter 160: loss = -5.321\n",
      "iter 161: loss = -5.321\n",
      "iter 162: loss = -5.620\n",
      "iter 163: loss = -5.328\n",
      "iter 164: loss = -5.418\n",
      "iter 165: loss = -5.444\n",
      "iter 166: loss = -5.114\n",
      "iter 167: loss = -5.421\n",
      "iter 168: loss = -5.131\n",
      "iter 169: loss = -5.362\n",
      "iter 170: loss = -5.136\n",
      "iter 171: loss = -5.195\n",
      "iter 172: loss = -5.162\n",
      "iter 173: loss = -5.250\n",
      "iter 174: loss = -5.101\n",
      "iter 175: loss = -5.091\n",
      "iter 176: loss = -5.292\n",
      "iter 177: loss = -5.147\n",
      "iter 178: loss = -5.124\n",
      "iter 179: loss = -4.849\n",
      "iter 180: loss = -4.924\n",
      "iter 181: loss = -4.763\n",
      "iter 182: loss = -5.070\n",
      "iter 183: loss = -5.003\n",
      "iter 184: loss = -4.871\n",
      "iter 185: loss = -5.036\n",
      "iter 186: loss = -4.687\n",
      "iter 187: loss = -4.596\n",
      "iter 188: loss = -4.895\n",
      "iter 189: loss = -4.898\n",
      "iter 190: loss = -4.768\n",
      "iter 191: loss = -4.432\n",
      "iter 192: loss = -4.961\n",
      "iter 193: loss = -4.690\n",
      "iter 194: loss = -4.756\n",
      "iter 195: loss = -4.814\n",
      "iter 196: loss = -4.283\n",
      "iter 197: loss = -4.735\n",
      "iter 198: loss = -4.712\n",
      "iter 199: loss = -4.562\n",
      "iter 200: loss = -4.655\n",
      "iter 201: loss = -4.560\n",
      "iter 202: loss = -4.135\n",
      "iter 203: loss = -4.532\n",
      "iter 204: loss = -4.351\n",
      "iter 205: loss = -4.453\n",
      "iter 206: loss = -4.720\n",
      "iter 207: loss = -4.534\n",
      "iter 208: loss = -4.168\n",
      "iter 209: loss = -4.282\n",
      "iter 210: loss = -4.272\n",
      "iter 211: loss = -4.494\n",
      "iter 212: loss = -4.250\n",
      "iter 213: loss = -4.376\n",
      "iter 214: loss = -4.174\n",
      "iter 215: loss = -4.378\n",
      "iter 216: loss = -4.249\n",
      "iter 217: loss = -4.001\n",
      "iter 218: loss = -3.057\n",
      "iter 219: loss = -4.385\n",
      "iter 220: loss = -4.152\n",
      "iter 221: loss = -4.036\n",
      "iter 222: loss = -4.061\n",
      "iter 223: loss = -4.140\n",
      "iter 224: loss = -4.195\n",
      "iter 225: loss = -3.926\n",
      "iter 226: loss = -4.090\n",
      "iter 227: loss = -4.186\n",
      "iter 228: loss = -3.889\n",
      "iter 229: loss = -3.865\n",
      "iter 230: loss = -4.047\n",
      "iter 231: loss = -3.908\n",
      "iter 232: loss = -4.061\n",
      "iter 233: loss = -3.767\n",
      "iter 234: loss = -3.892\n",
      "iter 235: loss = -3.949\n",
      "iter 236: loss = -3.649\n",
      "iter 237: loss = -3.822\n",
      "iter 238: loss = -3.626\n",
      "iter 239: loss = -3.946\n",
      "iter 240: loss = -3.623\n",
      "iter 241: loss = -3.576\n",
      "iter 242: loss = -3.647\n",
      "iter 243: loss = -3.589\n",
      "iter 244: loss = -2.979\n",
      "iter 245: loss = -3.298\n",
      "iter 246: loss = -3.291\n",
      "iter 247: loss = -3.745\n",
      "iter 248: loss = -2.932\n",
      "iter 249: loss = -3.417\n",
      "iter 250: loss = -3.430\n",
      "iter 251: loss = -3.262\n",
      "iter 252: loss = -3.490\n",
      "iter 253: loss = -2.981\n",
      "iter 254: loss = -3.216\n",
      "iter 255: loss = -3.186\n",
      "iter 256: loss = -3.048\n",
      "iter 257: loss = -3.435\n",
      "iter 258: loss = -3.736\n",
      "iter 259: loss = -3.217\n",
      "iter 260: loss = -3.031\n",
      "iter 261: loss = -3.162\n",
      "iter 262: loss = -3.077\n",
      "iter 263: loss = -2.947\n",
      "iter 264: loss = -3.044\n",
      "iter 265: loss = -3.054\n",
      "iter 266: loss = -2.892\n",
      "iter 267: loss = -2.881\n",
      "iter 268: loss = -3.079\n",
      "iter 269: loss = -2.860\n",
      "iter 270: loss = -3.148\n",
      "iter 271: loss = -2.989\n",
      "iter 272: loss = -2.955\n",
      "iter 273: loss = -2.965\n",
      "iter 274: loss = -2.713\n",
      "iter 275: loss = -2.468\n",
      "iter 276: loss = -2.813\n",
      "iter 277: loss = -2.680\n",
      "iter 278: loss = -2.752\n",
      "iter 279: loss = -2.138\n",
      "iter 280: loss = -2.887\n",
      "iter 281: loss = -2.740\n",
      "iter 282: loss = -2.504\n",
      "iter 283: loss = -2.651\n",
      "iter 284: loss = -2.539\n",
      "iter 285: loss = -2.473\n",
      "iter 286: loss = -2.376\n",
      "iter 287: loss = -2.503\n",
      "iter 288: loss = -2.329\n",
      "iter 289: loss = -2.416\n",
      "iter 290: loss = -2.388\n",
      "iter 291: loss = -0.955\n",
      "iter 292: loss = -2.256\n",
      "iter 293: loss = -2.239\n",
      "iter 294: loss = -1.987\n",
      "iter 295: loss = -2.156\n",
      "iter 296: loss = -2.245\n",
      "iter 297: loss = -2.275\n",
      "iter 298: loss = -2.160\n",
      "iter 299: loss = -2.133\n",
      "iter 300: loss = -1.777\n",
      "iter 301: loss = -2.059\n",
      "iter 302: loss = -1.020\n",
      "iter 303: loss = -0.935\n",
      "iter 304: loss = -1.746\n",
      "iter 305: loss = -1.596\n",
      "iter 306: loss = -1.761\n",
      "iter 307: loss = -1.618\n",
      "iter 308: loss = -1.069\n",
      "iter 309: loss = -1.574\n",
      "iter 310: loss = -1.789\n",
      "iter 311: loss = -1.638\n",
      "iter 312: loss = -1.757\n",
      "iter 313: loss = -1.295\n",
      "iter 314: loss = -1.332\n",
      "iter 315: loss = -1.726\n",
      "iter 316: loss = -1.350\n",
      "iter 317: loss = -1.597\n",
      "iter 318: loss = -1.622\n",
      "iter 319: loss = -0.729\n",
      "iter 320: loss = -1.571\n",
      "iter 321: loss = -1.421\n",
      "iter 322: loss = -1.442\n",
      "iter 323: loss = -1.486\n",
      "iter 324: loss = -1.314\n",
      "iter 325: loss = -1.175\n",
      "iter 326: loss = -1.135\n",
      "iter 327: loss = -1.229\n",
      "iter 328: loss = -0.759\n",
      "iter 329: loss = -1.088\n",
      "iter 330: loss = -0.701\n",
      "iter 331: loss = 0.387\n",
      "iter 332: loss = -0.398\n",
      "iter 333: loss = -0.469\n",
      "iter 334: loss = -0.028\n",
      "iter 335: loss = -1.035\n",
      "iter 336: loss = 0.023\n",
      "iter 337: loss = -1.145\n",
      "iter 338: loss = -0.769\n",
      "iter 339: loss = -0.574\n",
      "iter 340: loss = -0.652\n",
      "iter 341: loss = -0.510\n",
      "iter 342: loss = -0.777\n",
      "iter 343: loss = -0.674\n",
      "iter 344: loss = 0.453\n",
      "iter 345: loss = -0.332\n",
      "iter 346: loss = 0.143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 347: loss = 0.077\n",
      "iter 348: loss = 0.408\n",
      "iter 349: loss = 0.217\n",
      "iter 350: loss = 0.021\n",
      "iter 351: loss = -0.153\n",
      "iter 352: loss = -0.008\n",
      "iter 353: loss = -0.039\n",
      "iter 354: loss = 0.814\n",
      "iter 355: loss = 1.051\n",
      "iter 356: loss = 1.202\n",
      "iter 357: loss = -0.030\n",
      "iter 358: loss = 0.484\n",
      "iter 359: loss = 0.090\n",
      "iter 360: loss = 0.309\n",
      "iter 361: loss = -0.186\n",
      "iter 362: loss = 0.005\n",
      "iter 363: loss = 0.061\n",
      "iter 364: loss = -0.537\n",
      "iter 365: loss = -0.318\n",
      "iter 366: loss = 0.451\n",
      "iter 367: loss = 0.156\n",
      "iter 368: loss = 0.306\n",
      "iter 369: loss = 1.210\n",
      "iter 370: loss = 1.212\n",
      "iter 371: loss = 0.213\n",
      "iter 372: loss = 0.757\n",
      "iter 373: loss = -0.069\n",
      "iter 374: loss = -0.131\n",
      "iter 375: loss = -0.297\n",
      "iter 376: loss = 0.076\n",
      "iter 377: loss = 0.445\n",
      "iter 378: loss = 0.554\n",
      "iter 379: loss = 0.182\n",
      "iter 380: loss = 0.353\n",
      "iter 381: loss = 0.424\n",
      "iter 382: loss = 0.494\n",
      "iter 383: loss = 0.212\n",
      "iter 384: loss = 0.091\n",
      "iter 385: loss = 0.289\n",
      "iter 386: loss = 0.452\n",
      "iter 387: loss = 0.528\n",
      "iter 388: loss = 0.392\n",
      "iter 389: loss = 0.459\n",
      "iter 390: loss = 0.777\n",
      "iter 391: loss = 0.571\n",
      "iter 392: loss = 1.992\n",
      "iter 393: loss = 0.756\n",
      "iter 394: loss = 0.650\n",
      "iter 395: loss = 0.567\n",
      "iter 396: loss = 0.477\n",
      "iter 397: loss = 0.560\n",
      "iter 398: loss = 0.962\n",
      "iter 399: loss = 0.905\n",
      "iter 400: loss = 0.373\n",
      "iter 401: loss = 0.820\n",
      "iter 402: loss = 0.815\n",
      "iter 403: loss = 0.625\n",
      "iter 404: loss = 0.875\n",
      "iter 405: loss = 1.113\n",
      "iter 406: loss = 0.646\n",
      "iter 407: loss = 0.742\n",
      "iter 408: loss = 0.724\n",
      "iter 409: loss = 0.772\n",
      "iter 410: loss = 1.203\n",
      "iter 411: loss = 1.099\n",
      "iter 412: loss = 0.770\n",
      "iter 413: loss = 0.765\n",
      "iter 414: loss = 0.888\n",
      "iter 415: loss = 1.081\n",
      "iter 416: loss = 0.942\n",
      "iter 417: loss = 0.829\n",
      "iter 418: loss = 0.849\n",
      "iter 419: loss = 1.175\n",
      "iter 420: loss = 1.284\n",
      "iter 421: loss = 1.437\n",
      "iter 422: loss = 1.119\n",
      "iter 423: loss = 1.433\n",
      "iter 424: loss = 1.582\n",
      "iter 425: loss = 1.310\n",
      "iter 426: loss = 1.546\n",
      "iter 427: loss = 1.078\n",
      "iter 428: loss = 1.116\n",
      "iter 429: loss = 1.575\n",
      "iter 430: loss = 1.224\n",
      "iter 431: loss = 1.113\n",
      "iter 432: loss = 1.922\n",
      "iter 433: loss = 1.700\n",
      "iter 434: loss = 1.916\n",
      "iter 435: loss = 2.112\n",
      "iter 436: loss = 2.242\n",
      "iter 437: loss = 2.384\n",
      "iter 438: loss = 1.886\n",
      "iter 439: loss = 2.367\n",
      "iter 440: loss = 1.993\n",
      "iter 441: loss = 2.887\n",
      "iter 442: loss = 1.657\n",
      "iter 443: loss = 1.902\n",
      "iter 444: loss = 1.998\n",
      "iter 445: loss = 1.971\n",
      "iter 446: loss = 1.724\n",
      "iter 447: loss = 1.476\n",
      "iter 448: loss = 1.957\n",
      "iter 449: loss = 1.961\n",
      "iter 450: loss = 1.787\n",
      "iter 451: loss = 1.895\n",
      "iter 452: loss = 1.884\n",
      "iter 453: loss = 1.829\n",
      "iter 454: loss = 1.713\n",
      "iter 455: loss = 1.689\n",
      "iter 456: loss = 1.958\n",
      "iter 457: loss = 2.211\n",
      "iter 458: loss = 1.685\n",
      "iter 459: loss = 1.946\n",
      "iter 460: loss = 1.958\n",
      "iter 461: loss = 1.785\n",
      "iter 462: loss = 1.754\n",
      "iter 463: loss = 1.413\n",
      "iter 464: loss = 1.942\n",
      "iter 465: loss = 1.979\n",
      "iter 466: loss = 2.116\n",
      "iter 467: loss = 2.465\n",
      "iter 468: loss = 1.715\n",
      "iter 469: loss = 2.002\n",
      "iter 470: loss = 1.986\n",
      "iter 471: loss = 2.656\n",
      "iter 472: loss = 3.079\n",
      "iter 473: loss = 2.813\n",
      "iter 474: loss = 6.772\n",
      "iter 475: loss = 4.150\n",
      "iter 476: loss = 3.117\n",
      "iter 477: loss = 6.443\n",
      "iter 478: loss = 3.304\n",
      "iter 479: loss = 2.920\n",
      "iter 480: loss = 3.538\n",
      "iter 481: loss = 3.195\n",
      "iter 482: loss = 3.166\n",
      "iter 483: loss = 2.528\n",
      "iter 484: loss = 4.339\n",
      "iter 485: loss = 3.368\n",
      "iter 486: loss = 2.439\n",
      "iter 487: loss = 3.005\n",
      "iter 488: loss = 3.432\n",
      "iter 489: loss = 4.264\n",
      "iter 490: loss = 3.957\n",
      "iter 491: loss = 3.378\n",
      "iter 492: loss = 3.321\n",
      "iter 493: loss = 2.843\n",
      "iter 494: loss = 4.100\n",
      "iter 495: loss = 5.980\n",
      "iter 496: loss = 3.223\n",
      "iter 497: loss = 2.647\n",
      "iter 498: loss = 2.354\n",
      "iter 499: loss = 2.422\n",
      "iter 500: loss = 2.675\n",
      "iter 501: loss = 2.544\n",
      "iter 502: loss = 2.948\n",
      "iter 503: loss = 2.719\n",
      "iter 504: loss = 2.350\n",
      "iter 505: loss = 2.100\n",
      "iter 506: loss = 2.477\n",
      "iter 507: loss = 2.410\n",
      "iter 508: loss = 2.412\n",
      "iter 509: loss = 2.436\n",
      "iter 510: loss = 2.524\n",
      "iter 511: loss = 2.654\n",
      "iter 512: loss = 2.343\n",
      "iter 513: loss = 2.619\n",
      "iter 514: loss = 2.290\n",
      "iter 515: loss = 2.291\n",
      "iter 516: loss = 3.437\n",
      "iter 517: loss = 2.497\n",
      "iter 518: loss = 2.504\n",
      "iter 519: loss = 3.013\n",
      "iter 520: loss = 2.850\n",
      "iter 521: loss = 2.894\n",
      "iter 522: loss = 2.344\n",
      "iter 523: loss = 2.239\n",
      "iter 524: loss = 2.805\n",
      "iter 525: loss = 2.959\n",
      "iter 526: loss = 2.388\n",
      "iter 527: loss = 2.886\n",
      "iter 528: loss = 2.747\n",
      "iter 529: loss = 2.574\n",
      "iter 530: loss = 2.589\n",
      "iter 531: loss = 2.441\n",
      "iter 532: loss = 2.514\n",
      "iter 533: loss = 2.663\n",
      "iter 534: loss = 2.777\n",
      "iter 535: loss = 2.454\n",
      "iter 536: loss = 3.361\n",
      "iter 537: loss = 2.717\n",
      "iter 538: loss = 2.320\n",
      "iter 539: loss = 3.186\n",
      "iter 540: loss = 2.523\n",
      "iter 541: loss = 3.107\n",
      "iter 542: loss = 3.047\n",
      "iter 543: loss = 2.630\n",
      "iter 544: loss = 3.140\n",
      "iter 545: loss = 2.497\n",
      "iter 546: loss = 3.254\n",
      "iter 547: loss = 2.632\n",
      "iter 548: loss = 2.871\n",
      "iter 549: loss = 2.881\n",
      "iter 550: loss = 3.777\n",
      "iter 551: loss = 2.671\n",
      "iter 552: loss = 2.816\n",
      "iter 553: loss = 2.831\n",
      "iter 554: loss = 3.191\n",
      "iter 555: loss = 2.769\n",
      "iter 556: loss = 2.947\n",
      "iter 557: loss = 2.606\n",
      "iter 558: loss = 2.479\n",
      "iter 559: loss = 3.137\n",
      "iter 560: loss = 2.785\n",
      "iter 561: loss = 2.729\n",
      "iter 562: loss = 3.188\n",
      "iter 563: loss = 3.049\n",
      "iter 564: loss = 2.900\n",
      "iter 565: loss = 2.755\n",
      "iter 566: loss = 2.919\n",
      "iter 567: loss = 3.141\n",
      "iter 568: loss = 3.401\n",
      "iter 569: loss = 3.461\n",
      "iter 570: loss = 2.741\n",
      "iter 571: loss = 3.140\n",
      "iter 572: loss = 2.784\n",
      "iter 573: loss = 3.233\n",
      "iter 574: loss = 3.491\n",
      "iter 575: loss = 2.943\n",
      "iter 576: loss = 2.707\n",
      "iter 577: loss = 3.079\n",
      "iter 578: loss = 3.240\n",
      "iter 579: loss = 2.668\n",
      "iter 580: loss = 3.162\n",
      "iter 581: loss = 3.332\n",
      "iter 582: loss = 3.357\n",
      "iter 583: loss = 3.263\n",
      "iter 584: loss = 2.984\n",
      "iter 585: loss = 2.926\n",
      "iter 586: loss = 3.779\n",
      "iter 587: loss = 2.980\n",
      "iter 588: loss = 3.007\n",
      "iter 589: loss = 3.118\n",
      "iter 590: loss = 2.669\n",
      "iter 591: loss = 3.183\n",
      "iter 592: loss = 2.978\n",
      "iter 593: loss = 3.314\n",
      "iter 594: loss = 3.358\n",
      "iter 595: loss = 3.132\n",
      "iter 596: loss = 2.953\n",
      "iter 597: loss = 3.038\n",
      "iter 598: loss = 2.930\n",
      "iter 599: loss = 3.515\n",
      "iter 600: loss = 3.322\n",
      "iter 601: loss = 4.537\n",
      "iter 602: loss = 3.575\n",
      "iter 603: loss = 3.821\n",
      "iter 604: loss = 3.699\n",
      "iter 605: loss = 3.435\n",
      "iter 606: loss = 3.193\n",
      "iter 607: loss = 3.491\n",
      "iter 608: loss = 3.194\n",
      "iter 609: loss = 2.989\n",
      "iter 610: loss = 3.540\n",
      "iter 611: loss = 3.775\n",
      "iter 612: loss = 3.094\n",
      "iter 613: loss = 3.342\n",
      "iter 614: loss = 3.329\n",
      "iter 615: loss = 3.403\n",
      "iter 616: loss = 3.523\n",
      "iter 617: loss = 3.296\n",
      "iter 618: loss = 3.499\n",
      "iter 619: loss = 3.690\n",
      "iter 620: loss = 3.345\n",
      "iter 621: loss = 4.734\n",
      "iter 622: loss = 3.571\n",
      "iter 623: loss = 3.549\n",
      "iter 624: loss = 3.800\n",
      "iter 625: loss = 3.585\n",
      "iter 626: loss = 3.791\n",
      "iter 627: loss = 3.652\n",
      "iter 628: loss = 3.915\n",
      "iter 629: loss = 3.710\n",
      "iter 630: loss = 3.853\n",
      "iter 631: loss = 3.911\n",
      "iter 632: loss = 3.512\n",
      "iter 633: loss = 5.731\n",
      "iter 634: loss = 4.253\n",
      "iter 635: loss = 3.346\n",
      "iter 636: loss = 4.057\n",
      "iter 637: loss = 3.764\n",
      "iter 638: loss = 3.527\n",
      "iter 639: loss = 3.913\n",
      "iter 640: loss = 4.737\n",
      "iter 641: loss = 3.754\n",
      "iter 642: loss = 3.750\n",
      "iter 643: loss = 3.576\n",
      "iter 644: loss = 3.281\n",
      "iter 645: loss = 3.675\n",
      "iter 646: loss = 4.193\n",
      "iter 647: loss = 3.977\n",
      "iter 648: loss = 3.565\n",
      "iter 649: loss = 3.732\n",
      "iter 650: loss = 3.380\n",
      "iter 651: loss = 3.677\n",
      "iter 652: loss = 3.808\n",
      "iter 653: loss = 3.510\n",
      "iter 654: loss = 3.541\n",
      "iter 655: loss = 3.908\n",
      "iter 656: loss = 3.899\n",
      "iter 657: loss = 3.723\n",
      "iter 658: loss = 3.984\n",
      "iter 659: loss = 3.719\n",
      "iter 660: loss = 4.058\n",
      "iter 661: loss = 4.346\n",
      "iter 662: loss = 3.421\n",
      "iter 663: loss = 3.457\n",
      "iter 664: loss = 3.670\n",
      "iter 665: loss = 3.652\n",
      "iter 666: loss = 3.380\n",
      "iter 667: loss = 3.850\n",
      "iter 668: loss = 3.649\n",
      "iter 669: loss = 3.327\n",
      "iter 670: loss = 3.584\n",
      "iter 671: loss = 3.741\n",
      "iter 672: loss = 3.733\n",
      "iter 673: loss = 3.602\n",
      "iter 674: loss = 4.024\n",
      "iter 675: loss = 3.476\n",
      "iter 676: loss = 3.561\n",
      "iter 677: loss = 3.865\n",
      "iter 678: loss = 3.771\n",
      "iter 679: loss = 3.885\n",
      "iter 680: loss = 3.609\n",
      "iter 681: loss = 3.891\n",
      "iter 682: loss = 3.919\n",
      "iter 683: loss = 3.738\n",
      "iter 684: loss = 3.719\n",
      "iter 685: loss = 3.830\n",
      "iter 686: loss = 3.768\n",
      "iter 687: loss = 3.601\n",
      "iter 688: loss = 3.661\n",
      "iter 689: loss = 3.244\n",
      "iter 690: loss = 3.809\n",
      "iter 691: loss = 4.272\n",
      "iter 692: loss = 3.837\n",
      "iter 693: loss = 3.766\n",
      "iter 694: loss = 3.776\n",
      "iter 695: loss = 3.766\n",
      "iter 696: loss = 3.579\n",
      "iter 697: loss = 3.774\n",
      "iter 698: loss = 3.890\n",
      "iter 699: loss = 4.025\n",
      "iter 700: loss = 3.881\n",
      "iter 701: loss = 3.825\n",
      "iter 702: loss = 3.876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 703: loss = 3.655\n",
      "iter 704: loss = 3.777\n",
      "iter 705: loss = 3.674\n",
      "iter 706: loss = 3.895\n",
      "iter 707: loss = 3.781\n",
      "iter 708: loss = 3.648\n",
      "iter 709: loss = 3.914\n",
      "iter 710: loss = 4.089\n",
      "iter 711: loss = 4.673\n",
      "iter 712: loss = 4.454\n",
      "iter 713: loss = 4.480\n",
      "iter 714: loss = 4.386\n",
      "iter 715: loss = 4.171\n",
      "iter 716: loss = 3.865\n",
      "iter 717: loss = 4.194\n",
      "iter 718: loss = 4.049\n",
      "iter 719: loss = 4.466\n",
      "iter 720: loss = 4.384\n",
      "iter 721: loss = 3.904\n",
      "iter 722: loss = 4.056\n",
      "iter 723: loss = 3.709\n",
      "iter 724: loss = 3.894\n",
      "iter 725: loss = 4.334\n",
      "iter 726: loss = 3.995\n",
      "iter 727: loss = 4.550\n",
      "iter 728: loss = 4.341\n",
      "iter 729: loss = 4.434\n",
      "iter 730: loss = 5.103\n",
      "iter 731: loss = 4.189\n",
      "iter 732: loss = 4.208\n",
      "iter 733: loss = 4.144\n",
      "iter 734: loss = 4.512\n",
      "iter 735: loss = 4.208\n",
      "iter 736: loss = 4.760\n",
      "iter 737: loss = 4.968\n",
      "iter 738: loss = 4.253\n",
      "iter 739: loss = 4.125\n",
      "iter 740: loss = 4.275\n",
      "iter 741: loss = 4.176\n",
      "iter 742: loss = 4.225\n",
      "iter 743: loss = 4.209\n",
      "iter 744: loss = 4.397\n",
      "iter 745: loss = 4.921\n",
      "iter 746: loss = 4.122\n",
      "iter 747: loss = 4.711\n",
      "iter 748: loss = 4.842\n",
      "iter 749: loss = 5.010\n",
      "iter 750: loss = 4.444\n",
      "iter 751: loss = 4.199\n",
      "iter 752: loss = 4.396\n",
      "iter 753: loss = 4.266\n",
      "iter 754: loss = 4.533\n",
      "iter 755: loss = 4.350\n",
      "iter 756: loss = 4.588\n",
      "iter 757: loss = 4.517\n",
      "iter 758: loss = 4.498\n",
      "iter 759: loss = 4.414\n",
      "iter 760: loss = 4.858\n",
      "iter 761: loss = 4.834\n",
      "iter 762: loss = 5.377\n",
      "iter 763: loss = 4.392\n",
      "iter 764: loss = 4.416\n",
      "iter 765: loss = 4.514\n",
      "iter 766: loss = 4.809\n",
      "iter 767: loss = 4.282\n",
      "iter 768: loss = 4.240\n",
      "iter 769: loss = 4.244\n",
      "iter 770: loss = 4.606\n",
      "iter 771: loss = 4.671\n",
      "iter 772: loss = 4.298\n",
      "iter 773: loss = 4.626\n",
      "iter 774: loss = 4.074\n",
      "iter 775: loss = 4.594\n",
      "iter 776: loss = 4.608\n",
      "iter 777: loss = 4.274\n",
      "iter 778: loss = 4.594\n",
      "iter 779: loss = 5.497\n",
      "iter 780: loss = 4.335\n",
      "iter 781: loss = 4.076\n",
      "iter 782: loss = 4.340\n",
      "iter 783: loss = 4.101\n",
      "iter 784: loss = 4.548\n",
      "iter 785: loss = 4.111\n",
      "iter 786: loss = 5.754\n",
      "iter 787: loss = 4.627\n",
      "iter 788: loss = 4.961\n",
      "iter 789: loss = 5.324\n",
      "iter 790: loss = 4.624\n",
      "iter 791: loss = 4.502\n",
      "iter 792: loss = 4.428\n",
      "iter 793: loss = 4.325\n",
      "iter 794: loss = 4.377\n",
      "iter 795: loss = 4.538\n",
      "iter 796: loss = 4.493\n",
      "iter 797: loss = 4.835\n",
      "iter 798: loss = 4.135\n",
      "iter 799: loss = 4.529\n",
      "iter 800: loss = 4.858\n",
      "iter 801: loss = 4.252\n",
      "iter 802: loss = 4.595\n",
      "iter 803: loss = 4.378\n",
      "iter 804: loss = 4.704\n",
      "iter 805: loss = 4.650\n",
      "iter 806: loss = 4.519\n",
      "iter 807: loss = 4.319\n",
      "iter 808: loss = 4.392\n",
      "iter 809: loss = 4.417\n",
      "iter 810: loss = 4.763\n",
      "iter 811: loss = 4.835\n",
      "iter 812: loss = 4.772\n",
      "iter 813: loss = 4.603\n",
      "iter 814: loss = 4.682\n",
      "iter 815: loss = 5.578\n",
      "iter 816: loss = 5.167\n",
      "iter 817: loss = 4.621\n",
      "iter 818: loss = 5.177\n",
      "iter 819: loss = 4.939\n",
      "iter 820: loss = 4.476\n",
      "iter 821: loss = 4.332\n",
      "iter 822: loss = 4.655\n",
      "iter 823: loss = 4.551\n",
      "iter 824: loss = 4.413\n",
      "iter 825: loss = 4.420\n",
      "iter 826: loss = 4.602\n",
      "iter 827: loss = 5.079\n",
      "iter 828: loss = 4.723\n",
      "iter 829: loss = 4.356\n",
      "iter 830: loss = 4.205\n",
      "iter 831: loss = 4.315\n",
      "iter 832: loss = 4.426\n",
      "iter 833: loss = 4.531\n",
      "iter 834: loss = 4.731\n",
      "iter 835: loss = 4.472\n",
      "iter 836: loss = 4.966\n",
      "iter 837: loss = 4.484\n",
      "iter 838: loss = 4.467\n",
      "iter 839: loss = 5.340\n",
      "iter 840: loss = 4.996\n",
      "iter 841: loss = 4.587\n",
      "iter 842: loss = 5.121\n",
      "iter 843: loss = 4.896\n",
      "iter 844: loss = 4.462\n",
      "iter 845: loss = 4.273\n",
      "iter 846: loss = 4.388\n",
      "iter 847: loss = 4.720\n",
      "iter 848: loss = 4.254\n",
      "iter 849: loss = 4.321\n",
      "iter 850: loss = 4.473\n",
      "iter 851: loss = 4.591\n",
      "iter 852: loss = 4.321\n",
      "iter 853: loss = 5.083\n",
      "iter 854: loss = 4.511\n",
      "iter 855: loss = 4.400\n",
      "iter 856: loss = 4.406\n",
      "iter 857: loss = 4.796\n",
      "iter 858: loss = 4.541\n",
      "iter 859: loss = 4.709\n",
      "iter 860: loss = 4.808\n",
      "iter 861: loss = 4.694\n",
      "iter 862: loss = 4.663\n",
      "iter 863: loss = 4.714\n",
      "iter 864: loss = 4.956\n",
      "iter 865: loss = 4.991\n",
      "iter 866: loss = 4.564\n",
      "iter 867: loss = 4.614\n",
      "iter 868: loss = 4.432\n",
      "iter 869: loss = 4.826\n",
      "iter 870: loss = 4.453\n",
      "iter 871: loss = 4.392\n",
      "iter 872: loss = 4.608\n",
      "iter 873: loss = 4.997\n",
      "iter 874: loss = 4.937\n",
      "iter 875: loss = 4.717\n",
      "iter 876: loss = 4.478\n",
      "iter 877: loss = 4.422\n",
      "iter 878: loss = 4.431\n",
      "iter 879: loss = 4.615\n",
      "iter 880: loss = 4.433\n",
      "iter 881: loss = 4.496\n",
      "iter 882: loss = 4.442\n",
      "iter 883: loss = 4.600\n",
      "iter 884: loss = 4.613\n",
      "iter 885: loss = 4.435\n",
      "iter 886: loss = 4.531\n",
      "iter 887: loss = 4.587\n",
      "iter 888: loss = 4.672\n",
      "iter 889: loss = 4.745\n",
      "iter 890: loss = 4.463\n",
      "iter 891: loss = 4.436\n",
      "iter 892: loss = 4.486\n",
      "iter 893: loss = 4.476\n",
      "iter 894: loss = 4.297\n",
      "iter 895: loss = 4.471\n",
      "iter 896: loss = 4.397\n",
      "iter 897: loss = 4.465\n",
      "iter 898: loss = 4.382\n",
      "iter 899: loss = 4.730\n",
      "iter 900: loss = 4.284\n",
      "iter 901: loss = 4.360\n",
      "iter 902: loss = 4.827\n",
      "iter 903: loss = 4.638\n",
      "iter 904: loss = 4.598\n",
      "iter 905: loss = 4.424\n",
      "iter 906: loss = 4.400\n",
      "iter 907: loss = 4.808\n",
      "iter 908: loss = 4.564\n",
      "iter 909: loss = 4.640\n",
      "iter 910: loss = 4.549\n",
      "iter 911: loss = 4.585\n",
      "iter 912: loss = 4.791\n",
      "iter 913: loss = 4.509\n",
      "iter 914: loss = 4.766\n",
      "iter 915: loss = 5.077\n",
      "iter 916: loss = 4.859\n",
      "iter 917: loss = 4.573\n",
      "iter 918: loss = 4.579\n",
      "iter 919: loss = 4.683\n",
      "iter 920: loss = 4.455\n",
      "iter 921: loss = 4.755\n",
      "iter 922: loss = 4.522\n",
      "iter 923: loss = 4.480\n",
      "iter 924: loss = 4.659\n",
      "iter 925: loss = 4.556\n",
      "iter 926: loss = 4.681\n",
      "iter 927: loss = 4.499\n",
      "iter 928: loss = 4.647\n",
      "iter 929: loss = 4.507\n",
      "iter 930: loss = 4.470\n",
      "iter 931: loss = 4.798\n",
      "iter 932: loss = 4.414\n",
      "iter 933: loss = 4.737\n",
      "iter 934: loss = 4.533\n",
      "iter 935: loss = 4.739\n",
      "iter 936: loss = 4.514\n",
      "iter 937: loss = 4.790\n",
      "iter 938: loss = 4.751\n",
      "iter 939: loss = 4.794\n",
      "iter 940: loss = 4.604\n",
      "iter 941: loss = 4.698\n",
      "iter 942: loss = 4.482\n",
      "iter 943: loss = 4.696\n",
      "iter 944: loss = 4.737\n",
      "iter 945: loss = 4.865\n",
      "iter 946: loss = 4.582\n",
      "iter 947: loss = 4.733\n",
      "iter 948: loss = 4.636\n",
      "iter 949: loss = 4.812\n",
      "iter 950: loss = 4.671\n",
      "iter 951: loss = 4.896\n",
      "iter 952: loss = 4.788\n",
      "iter 953: loss = 4.614\n",
      "iter 954: loss = 4.755\n",
      "iter 955: loss = 4.661\n",
      "iter 956: loss = 4.750\n",
      "iter 957: loss = 5.434\n",
      "iter 958: loss = 4.785\n",
      "iter 959: loss = 4.760\n",
      "iter 960: loss = 4.778\n",
      "iter 961: loss = 4.773\n",
      "iter 962: loss = 4.870\n",
      "iter 963: loss = 5.081\n",
      "iter 964: loss = 4.958\n",
      "iter 965: loss = 4.929\n",
      "iter 966: loss = 4.935\n",
      "iter 967: loss = 4.661\n",
      "iter 968: loss = 4.669\n",
      "iter 969: loss = 4.785\n",
      "iter 970: loss = 4.768\n",
      "iter 971: loss = 4.856\n",
      "iter 972: loss = 4.687\n",
      "iter 973: loss = 4.850\n",
      "iter 974: loss = 4.731\n",
      "iter 975: loss = 4.813\n",
      "iter 976: loss = 4.627\n",
      "iter 977: loss = 4.694\n",
      "iter 978: loss = 4.791\n",
      "iter 979: loss = 4.986\n",
      "iter 980: loss = 5.363\n",
      "iter 981: loss = 4.973\n",
      "iter 982: loss = 5.114\n",
      "iter 983: loss = 5.775\n",
      "iter 984: loss = 5.709\n",
      "iter 985: loss = 4.997\n",
      "iter 986: loss = 4.885\n",
      "iter 987: loss = 4.797\n",
      "iter 988: loss = 5.327\n",
      "iter 989: loss = 4.908\n",
      "iter 990: loss = 4.917\n",
      "iter 991: loss = 5.249\n",
      "iter 992: loss = 5.172\n",
      "iter 993: loss = 5.323\n",
      "iter 994: loss = 6.604\n",
      "iter 995: loss = 5.869\n",
      "iter 996: loss = 7.082\n",
      "iter 997: loss = 5.858\n",
      "iter 998: loss = 5.043\n",
      "iter 999: loss = 5.248\n"
     ]
    }
   ],
   "source": [
    "Explore_DoubleWell.configure(epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gUTuXLjuHpb3"
   },
   "outputs": [],
   "source": [
    "x=BG.sample(10000)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "TLx1DVERV0LU",
    "outputId": "786c809c-2d64-4110-f98d-a5a53237e565"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax=plt.gca()\n",
    "ax.hist2d(x[:,0],x[:,1],bins=100,norm=matplotlib.colors.LogNorm())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQYkrIUVV-J6",
    "outputId": "f6bb6593-2bf3-454e-a920-28452556f66b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6884765625,\n",
       " 0.9345703125,\n",
       " 0.951171875,\n",
       " 0.9345703125,\n",
       " 0.9482421875,\n",
       " 0.9345703125,\n",
       " 0.99609375,\n",
       " 0.95703125,\n",
       " 0.955078125,\n",
       " 0.953125,\n",
       " 0.912109375,\n",
       " 0.951171875,\n",
       " 0.9736328125,\n",
       " 0.970703125,\n",
       " 0.9677734375,\n",
       " 0.97265625,\n",
       " 0.892578125,\n",
       " 0.96484375,\n",
       " 0.96875,\n",
       " 0.9755859375,\n",
       " 0.86328125,\n",
       " 0.9365234375,\n",
       " 0.935546875,\n",
       " 0.939453125,\n",
       " 0.9501953125,\n",
       " 0.955078125,\n",
       " 0.955078125,\n",
       " 0.935546875,\n",
       " 0.9423828125,\n",
       " 0.9365234375,\n",
       " 0.9453125,\n",
       " 0.939453125,\n",
       " 0.9345703125,\n",
       " 0.943359375,\n",
       " 0.935546875,\n",
       " 0.939453125,\n",
       " 0.72265625,\n",
       " 0.943359375,\n",
       " 0.9384765625,\n",
       " 0.9375,\n",
       " 0.974609375,\n",
       " 0.978515625,\n",
       " 0.9765625,\n",
       " 0.9697265625,\n",
       " 0.9521484375,\n",
       " 0.9619140625,\n",
       " 0.9375,\n",
       " 0.9248046875,\n",
       " 0.93359375,\n",
       " 0.9228515625,\n",
       " 0.93359375,\n",
       " 0.935546875,\n",
       " 0.93359375,\n",
       " 0.9287109375,\n",
       " 0.9287109375,\n",
       " 0.9189453125,\n",
       " 0.9365234375,\n",
       " 0.9189453125,\n",
       " 0.9033203125,\n",
       " 0.9208984375,\n",
       " 0.916015625,\n",
       " 0.91015625,\n",
       " 0.931640625,\n",
       " 0.9248046875,\n",
       " 0.9072265625,\n",
       " 0.921875,\n",
       " 0.921875,\n",
       " 0.916015625,\n",
       " 0.8955078125,\n",
       " 0.890625,\n",
       " 0.9228515625,\n",
       " 0.921875,\n",
       " 0.9208984375,\n",
       " 0.8974609375,\n",
       " 0.9189453125,\n",
       " 0.931640625,\n",
       " 0.9150390625,\n",
       " 0.92578125,\n",
       " 0.9169921875,\n",
       " 0.8984375,\n",
       " 0.900390625,\n",
       " 0.9140625,\n",
       " 0.92578125,\n",
       " 0.9208984375,\n",
       " 0.91796875,\n",
       " 0.9033203125,\n",
       " 0.900390625,\n",
       " 0.904296875,\n",
       " 0.9013671875,\n",
       " 0.8984375,\n",
       " 0.9072265625,\n",
       " 0.904296875,\n",
       " 0.9013671875,\n",
       " 0.8935546875,\n",
       " 0.900390625,\n",
       " 0.9140625,\n",
       " 0.8837890625,\n",
       " 0.8994140625,\n",
       " 0.8896484375,\n",
       " 0.89453125,\n",
       " 0.8984375,\n",
       " 0.904296875,\n",
       " 0.8935546875,\n",
       " 0.9169921875,\n",
       " 0.8798828125,\n",
       " 0.890625,\n",
       " 0.9013671875,\n",
       " 0.89453125,\n",
       " 0.8994140625,\n",
       " 0.896484375,\n",
       " 0.8916015625,\n",
       " 0.9013671875,\n",
       " 0.8720703125,\n",
       " 0.8994140625,\n",
       " 0.9013671875,\n",
       " 0.8896484375,\n",
       " 0.8935546875,\n",
       " 0.892578125,\n",
       " 0.90625,\n",
       " 0.8984375,\n",
       " 0.8759765625,\n",
       " 0.8857421875,\n",
       " 0.89453125,\n",
       " 0.888671875,\n",
       " 0.904296875,\n",
       " 0.9013671875,\n",
       " 0.8759765625,\n",
       " 0.884765625,\n",
       " 0.8779296875,\n",
       " 0.8935546875,\n",
       " 0.890625,\n",
       " 0.880859375,\n",
       " 0.865234375,\n",
       " 0.8896484375,\n",
       " 0.8818359375,\n",
       " 0.86328125,\n",
       " 0.8974609375,\n",
       " 0.8779296875,\n",
       " 0.8740234375,\n",
       " 0.9013671875,\n",
       " 0.890625,\n",
       " 0.87109375,\n",
       " 0.865234375,\n",
       " 0.8837890625,\n",
       " 0.8720703125,\n",
       " 0.884765625,\n",
       " 0.8818359375,\n",
       " 0.8740234375,\n",
       " 0.8701171875,\n",
       " 0.8759765625,\n",
       " 0.87890625,\n",
       " 0.8857421875,\n",
       " 0.8662109375,\n",
       " 0.87890625,\n",
       " 0.8720703125,\n",
       " 0.8984375,\n",
       " 0.8603515625,\n",
       " 0.8671875,\n",
       " 0.876953125,\n",
       " 0.8681640625,\n",
       " 0.8681640625,\n",
       " 0.8662109375,\n",
       " 0.8779296875,\n",
       " 0.8818359375,\n",
       " 0.849609375,\n",
       " 0.8828125,\n",
       " 0.8603515625,\n",
       " 0.86328125,\n",
       " 0.865234375,\n",
       " 0.8701171875,\n",
       " 0.8583984375,\n",
       " 0.8681640625,\n",
       " 0.8544921875,\n",
       " 0.880859375,\n",
       " 0.87890625,\n",
       " 0.86328125,\n",
       " 0.892578125,\n",
       " 0.8759765625,\n",
       " 0.873046875,\n",
       " 0.875,\n",
       " 0.853515625,\n",
       " 0.8720703125,\n",
       " 0.8681640625,\n",
       " 0.8681640625,\n",
       " 0.8583984375,\n",
       " 0.8671875,\n",
       " 0.8408203125,\n",
       " 0.859375,\n",
       " 0.880859375,\n",
       " 0.8740234375,\n",
       " 0.865234375,\n",
       " 0.8662109375,\n",
       " 0.8583984375,\n",
       " 0.87109375,\n",
       " 0.8466796875,\n",
       " 0.85546875,\n",
       " 0.85546875,\n",
       " 0.8818359375,\n",
       " 0.8583984375,\n",
       " 0.859375,\n",
       " 0.8369140625,\n",
       " 0.85546875,\n",
       " 0.84765625,\n",
       " 0.8466796875,\n",
       " 0.8642578125,\n",
       " 0.86328125,\n",
       " 0.8505859375,\n",
       " 0.8671875,\n",
       " 0.8623046875,\n",
       " 0.837890625,\n",
       " 0.86328125,\n",
       " 0.873046875,\n",
       " 0.853515625,\n",
       " 0.84765625,\n",
       " 0.849609375,\n",
       " 0.8564453125,\n",
       " 0.8564453125,\n",
       " 0.84375,\n",
       " 0.8583984375,\n",
       " 0.8603515625,\n",
       " 0.853515625,\n",
       " 0.8212890625,\n",
       " 0.830078125,\n",
       " 0.8583984375,\n",
       " 0.845703125,\n",
       " 0.8583984375,\n",
       " 0.8359375,\n",
       " 0.8642578125,\n",
       " 0.8544921875,\n",
       " 0.845703125,\n",
       " 0.84765625,\n",
       " 0.88671875,\n",
       " 0.8564453125,\n",
       " 0.869140625,\n",
       " 0.8330078125,\n",
       " 0.8623046875,\n",
       " 0.84375,\n",
       " 0.84765625,\n",
       " 0.8720703125,\n",
       " 0.8642578125,\n",
       " 0.884765625,\n",
       " 0.85546875,\n",
       " 0.853515625,\n",
       " 0.876953125,\n",
       " 0.876953125,\n",
       " 0.849609375,\n",
       " 0.8759765625,\n",
       " 0.8447265625,\n",
       " 0.8447265625,\n",
       " 0.8623046875,\n",
       " 0.876953125,\n",
       " 0.8544921875,\n",
       " 0.861328125,\n",
       " 0.837890625,\n",
       " 0.869140625,\n",
       " 0.861328125,\n",
       " 0.8427734375,\n",
       " 0.8642578125,\n",
       " 0.8544921875,\n",
       " 0.8310546875,\n",
       " 0.8369140625,\n",
       " 0.8447265625,\n",
       " 0.8544921875,\n",
       " 0.8642578125,\n",
       " 0.8369140625,\n",
       " 0.84765625,\n",
       " 0.86328125,\n",
       " 0.8515625,\n",
       " 0.84765625,\n",
       " 0.8251953125,\n",
       " 0.8486328125,\n",
       " 0.8310546875,\n",
       " 0.8271484375,\n",
       " 0.8291015625,\n",
       " 0.8447265625,\n",
       " 0.8583984375,\n",
       " 0.8505859375,\n",
       " 0.8427734375,\n",
       " 0.8369140625,\n",
       " 0.84765625,\n",
       " 0.8310546875,\n",
       " 0.8271484375,\n",
       " 0.849609375,\n",
       " 0.8291015625,\n",
       " 0.8349609375,\n",
       " 0.84765625,\n",
       " 0.8427734375,\n",
       " 0.849609375,\n",
       " 0.8544921875,\n",
       " 0.841796875,\n",
       " 0.85546875,\n",
       " 0.865234375,\n",
       " 0.8427734375,\n",
       " 0.861328125,\n",
       " 0.8349609375,\n",
       " 0.837890625,\n",
       " 0.85546875,\n",
       " 0.837890625,\n",
       " 0.859375,\n",
       " 0.814453125,\n",
       " 0.826171875,\n",
       " 0.8544921875,\n",
       " 0.830078125,\n",
       " 0.8544921875,\n",
       " 0.8681640625,\n",
       " 0.849609375,\n",
       " 0.826171875,\n",
       " 0.859375,\n",
       " 0.8564453125,\n",
       " 0.8603515625,\n",
       " 0.822265625,\n",
       " 0.85546875,\n",
       " 0.857421875,\n",
       " 0.830078125,\n",
       " 0.82421875,\n",
       " 0.8505859375,\n",
       " 0.865234375,\n",
       " 0.84375,\n",
       " 0.8525390625,\n",
       " 0.8408203125,\n",
       " 0.849609375,\n",
       " 0.865234375,\n",
       " 0.837890625,\n",
       " 0.8583984375,\n",
       " 0.8359375,\n",
       " 0.84375,\n",
       " 0.86328125,\n",
       " 0.8466796875,\n",
       " 0.84375,\n",
       " 0.857421875,\n",
       " 0.84765625,\n",
       " 0.8935546875,\n",
       " 0.845703125,\n",
       " 0.8486328125,\n",
       " 0.8662109375,\n",
       " 0.857421875,\n",
       " 0.86328125,\n",
       " 0.8583984375,\n",
       " 0.8486328125,\n",
       " 0.8447265625,\n",
       " 0.8662109375,\n",
       " 0.8740234375,\n",
       " 0.8623046875,\n",
       " 0.853515625,\n",
       " 0.8544921875,\n",
       " 0.8671875,\n",
       " 0.8818359375,\n",
       " 0.8642578125,\n",
       " 0.85546875,\n",
       " 0.888671875,\n",
       " 0.8818359375,\n",
       " 0.8798828125,\n",
       " 0.8720703125,\n",
       " 0.8662109375,\n",
       " 0.8876953125,\n",
       " 0.880859375,\n",
       " 0.8740234375,\n",
       " 0.8671875,\n",
       " 0.8935546875,\n",
       " 0.8740234375,\n",
       " 0.888671875,\n",
       " 0.8759765625,\n",
       " 0.8720703125,\n",
       " 0.876953125,\n",
       " 0.8662109375,\n",
       " 0.8583984375,\n",
       " 0.8515625,\n",
       " 0.8671875,\n",
       " 0.8603515625,\n",
       " 0.85546875,\n",
       " 0.853515625,\n",
       " 0.8720703125,\n",
       " 0.8955078125,\n",
       " 0.8828125,\n",
       " 0.8779296875,\n",
       " 0.845703125,\n",
       " 0.861328125,\n",
       " 0.8583984375,\n",
       " 0.8642578125,\n",
       " 0.8515625,\n",
       " 0.8623046875,\n",
       " 0.876953125,\n",
       " 0.875,\n",
       " 0.8505859375,\n",
       " 0.8720703125,\n",
       " 0.8662109375,\n",
       " 0.8623046875,\n",
       " 0.8759765625,\n",
       " 0.8681640625,\n",
       " 0.8486328125,\n",
       " 0.841796875,\n",
       " 0.859375,\n",
       " 0.857421875,\n",
       " 0.8583984375,\n",
       " 0.853515625,\n",
       " 0.8642578125,\n",
       " 0.8583984375,\n",
       " 0.8515625,\n",
       " 0.8818359375,\n",
       " 0.873046875,\n",
       " 0.86328125,\n",
       " 0.857421875,\n",
       " 0.8603515625,\n",
       " 0.869140625,\n",
       " 0.876953125,\n",
       " 0.869140625,\n",
       " 0.865234375,\n",
       " 0.8466796875,\n",
       " 0.8662109375,\n",
       " 0.8671875,\n",
       " 0.853515625,\n",
       " 0.8720703125,\n",
       " 0.865234375,\n",
       " 0.8798828125,\n",
       " 0.85546875,\n",
       " 0.8330078125,\n",
       " 0.8740234375,\n",
       " 0.865234375,\n",
       " 0.8701171875,\n",
       " 0.8740234375,\n",
       " 0.8779296875,\n",
       " 0.8740234375,\n",
       " 0.8515625,\n",
       " 0.8876953125,\n",
       " 0.8671875,\n",
       " 0.888671875,\n",
       " 0.8818359375,\n",
       " 0.8828125,\n",
       " 0.88671875,\n",
       " 0.8583984375,\n",
       " 0.8525390625,\n",
       " 0.8515625,\n",
       " 0.8759765625,\n",
       " 0.849609375,\n",
       " 0.8603515625,\n",
       " 0.8798828125,\n",
       " 0.8876953125,\n",
       " 0.888671875,\n",
       " 0.880859375,\n",
       " 0.8955078125,\n",
       " 0.892578125,\n",
       " 0.87109375,\n",
       " 0.8701171875,\n",
       " 0.8916015625,\n",
       " 0.87890625,\n",
       " 0.90234375,\n",
       " 0.8662109375,\n",
       " 0.892578125,\n",
       " 0.888671875,\n",
       " 0.8798828125,\n",
       " 0.87890625,\n",
       " 0.9052734375,\n",
       " 0.9140625,\n",
       " 0.8818359375,\n",
       " 0.8681640625,\n",
       " 0.8662109375,\n",
       " 0.8720703125,\n",
       " 0.8984375,\n",
       " 0.890625,\n",
       " 0.890625,\n",
       " 0.8564453125,\n",
       " 0.8623046875,\n",
       " 0.884765625,\n",
       " 0.892578125,\n",
       " 0.9091796875,\n",
       " 0.8876953125,\n",
       " 0.892578125,\n",
       " 0.8798828125,\n",
       " 0.880859375,\n",
       " 0.8916015625,\n",
       " 0.8974609375,\n",
       " 0.888671875,\n",
       " 0.9072265625,\n",
       " 0.892578125,\n",
       " 0.9111328125,\n",
       " 0.9091796875,\n",
       " 0.8896484375,\n",
       " 0.8955078125,\n",
       " 0.9033203125,\n",
       " 0.9140625,\n",
       " 0.9013671875,\n",
       " 0.9072265625,\n",
       " 0.9169921875,\n",
       " 0.8876953125,\n",
       " 0.9013671875,\n",
       " 0.9130859375,\n",
       " 0.9091796875,\n",
       " 0.890625,\n",
       " 0.8994140625,\n",
       " 0.9052734375,\n",
       " 0.916015625,\n",
       " 0.908203125,\n",
       " 0.9033203125,\n",
       " 0.90234375,\n",
       " 0.890625,\n",
       " 0.919921875,\n",
       " 0.923828125,\n",
       " 0.91796875,\n",
       " 0.9140625,\n",
       " 0.921875,\n",
       " 0.90625,\n",
       " 0.908203125,\n",
       " 0.9033203125,\n",
       " 0.9052734375,\n",
       " 0.9150390625,\n",
       " 0.9072265625,\n",
       " 0.9189453125,\n",
       " 0.921875,\n",
       " 0.919921875,\n",
       " 0.9208984375,\n",
       " 0.92578125,\n",
       " 0.912109375,\n",
       " 0.904296875,\n",
       " 0.908203125,\n",
       " 0.912109375,\n",
       " 0.916015625,\n",
       " 0.9296875,\n",
       " 0.916015625,\n",
       " 0.9208984375,\n",
       " 0.9345703125,\n",
       " 0.919921875,\n",
       " 0.912109375,\n",
       " 0.8876953125,\n",
       " 0.9404296875,\n",
       " 0.9033203125,\n",
       " 0.9052734375,\n",
       " 0.9150390625,\n",
       " 0.9248046875,\n",
       " 0.923828125,\n",
       " 0.9228515625,\n",
       " 0.9130859375,\n",
       " 0.9072265625,\n",
       " 0.9228515625,\n",
       " 0.9267578125,\n",
       " 0.91015625,\n",
       " 0.92578125,\n",
       " 0.921875,\n",
       " 0.8994140625,\n",
       " 0.912109375,\n",
       " 0.9248046875,\n",
       " 0.912109375,\n",
       " 0.9267578125,\n",
       " 0.9267578125,\n",
       " 0.9111328125,\n",
       " 0.9345703125,\n",
       " 0.908203125,\n",
       " 0.9306640625,\n",
       " 0.927734375,\n",
       " 0.9091796875,\n",
       " 0.9306640625,\n",
       " 0.9208984375,\n",
       " 0.9306640625,\n",
       " 0.9169921875,\n",
       " 0.935546875,\n",
       " 0.919921875,\n",
       " 0.9228515625,\n",
       " 0.9189453125,\n",
       " 0.927734375,\n",
       " 0.9189453125,\n",
       " 0.92578125,\n",
       " 0.9140625,\n",
       " 0.91015625,\n",
       " 0.9189453125,\n",
       " 0.9248046875,\n",
       " 0.9189453125,\n",
       " 0.9228515625,\n",
       " 0.923828125,\n",
       " 0.93359375,\n",
       " 0.9326171875,\n",
       " 0.9189453125,\n",
       " 0.9140625,\n",
       " 0.9111328125,\n",
       " 0.9150390625,\n",
       " 0.9248046875,\n",
       " 0.91796875,\n",
       " 0.9189453125,\n",
       " 0.919921875,\n",
       " 0.9091796875,\n",
       " 0.9296875,\n",
       " 0.93359375,\n",
       " 0.9365234375,\n",
       " 0.9443359375,\n",
       " 0.9111328125,\n",
       " 0.923828125,\n",
       " 0.9208984375,\n",
       " 0.9140625,\n",
       " 0.939453125,\n",
       " 0.927734375,\n",
       " 0.9248046875,\n",
       " 0.93359375,\n",
       " 0.927734375,\n",
       " 0.923828125,\n",
       " 0.927734375,\n",
       " 0.91015625,\n",
       " 0.9306640625,\n",
       " 0.9384765625,\n",
       " 0.9306640625,\n",
       " 0.939453125,\n",
       " 0.9345703125,\n",
       " 0.939453125,\n",
       " 0.9248046875,\n",
       " 0.9169921875,\n",
       " 0.923828125,\n",
       " 0.9375,\n",
       " 0.9287109375,\n",
       " 0.9375,\n",
       " 0.9208984375,\n",
       " 0.9287109375,\n",
       " 0.9228515625,\n",
       " 0.91796875,\n",
       " 0.93359375,\n",
       " 0.9482421875,\n",
       " 0.9306640625,\n",
       " 0.9208984375,\n",
       " 0.9345703125,\n",
       " 0.935546875,\n",
       " 0.9345703125,\n",
       " 0.9404296875,\n",
       " 0.9296875,\n",
       " 0.9365234375,\n",
       " 0.9169921875,\n",
       " 0.943359375,\n",
       " 0.9326171875,\n",
       " 0.9287109375,\n",
       " 0.921875,\n",
       " 0.93359375,\n",
       " 0.9345703125,\n",
       " 0.9462890625,\n",
       " 0.923828125,\n",
       " 0.9482421875,\n",
       " 0.9423828125,\n",
       " 0.939453125,\n",
       " 0.9521484375,\n",
       " 0.9296875,\n",
       " 0.931640625,\n",
       " 0.9482421875,\n",
       " 0.9453125,\n",
       " 0.9287109375,\n",
       " 0.9345703125,\n",
       " 0.9384765625,\n",
       " 0.931640625,\n",
       " 0.9306640625,\n",
       " 0.9267578125,\n",
       " 0.931640625,\n",
       " 0.935546875,\n",
       " 0.92578125,\n",
       " 0.9462890625,\n",
       " 0.9365234375,\n",
       " 0.9404296875,\n",
       " 0.923828125,\n",
       " 0.92578125,\n",
       " 0.9375,\n",
       " 0.9296875,\n",
       " 0.9375,\n",
       " 0.9248046875,\n",
       " 0.9296875,\n",
       " 0.93359375,\n",
       " 0.9404296875,\n",
       " 0.9375,\n",
       " 0.95703125,\n",
       " 0.93359375,\n",
       " 0.943359375,\n",
       " 0.9306640625,\n",
       " 0.9267578125,\n",
       " 0.9384765625,\n",
       " 0.9326171875,\n",
       " 0.9443359375,\n",
       " 0.943359375,\n",
       " 0.9306640625,\n",
       " 0.93359375,\n",
       " 0.93359375,\n",
       " 0.9306640625,\n",
       " 0.9375,\n",
       " 0.9150390625,\n",
       " 0.931640625,\n",
       " 0.931640625,\n",
       " 0.9375,\n",
       " 0.9404296875,\n",
       " 0.947265625,\n",
       " 0.9375,\n",
       " 0.93359375,\n",
       " 0.9365234375,\n",
       " 0.9326171875,\n",
       " 0.9228515625,\n",
       " 0.9228515625,\n",
       " 0.947265625,\n",
       " 0.9404296875,\n",
       " 0.931640625,\n",
       " 0.927734375,\n",
       " 0.9365234375,\n",
       " 0.92578125,\n",
       " 0.9326171875,\n",
       " 0.9404296875,\n",
       " 0.9306640625,\n",
       " 0.9326171875,\n",
       " 0.919921875,\n",
       " 0.9248046875,\n",
       " 0.9140625,\n",
       " 0.94140625,\n",
       " 0.9287109375,\n",
       " 0.9296875,\n",
       " 0.939453125,\n",
       " 0.9384765625,\n",
       " 0.9306640625,\n",
       " 0.9501953125,\n",
       " 0.9384765625,\n",
       " 0.9404296875,\n",
       " 0.9345703125,\n",
       " 0.9423828125,\n",
       " 0.9462890625,\n",
       " 0.9541015625,\n",
       " 0.9453125,\n",
       " 0.947265625,\n",
       " 0.94921875,\n",
       " 0.9375,\n",
       " 0.9482421875,\n",
       " 0.9462890625,\n",
       " 0.9375,\n",
       " 0.9521484375,\n",
       " 0.9501953125,\n",
       " 0.953125,\n",
       " 0.94140625,\n",
       " 0.9482421875,\n",
       " 0.939453125,\n",
       " 0.94921875,\n",
       " 0.935546875,\n",
       " 0.947265625,\n",
       " 0.9404296875,\n",
       " 0.9521484375,\n",
       " 0.9521484375,\n",
       " 0.9384765625,\n",
       " 0.9453125,\n",
       " 0.947265625,\n",
       " 0.955078125,\n",
       " 0.9501953125,\n",
       " 0.95703125,\n",
       " 0.9384765625,\n",
       " 0.9453125,\n",
       " 0.9560546875,\n",
       " 0.9619140625,\n",
       " 0.939453125,\n",
       " 0.9560546875,\n",
       " 0.951171875,\n",
       " 0.9501953125,\n",
       " 0.9580078125,\n",
       " 0.966796875,\n",
       " 0.947265625,\n",
       " 0.9453125,\n",
       " 0.9609375,\n",
       " 0.9609375,\n",
       " 0.966796875,\n",
       " 0.953125,\n",
       " 0.951171875,\n",
       " 0.9521484375,\n",
       " 0.9580078125,\n",
       " 0.9638671875,\n",
       " 0.9541015625,\n",
       " 0.951171875,\n",
       " 0.9658203125,\n",
       " 0.962890625,\n",
       " 0.9560546875,\n",
       " 0.966796875,\n",
       " 0.9580078125,\n",
       " 0.9384765625,\n",
       " 0.95703125,\n",
       " 0.9443359375,\n",
       " 0.955078125,\n",
       " 0.962890625,\n",
       " 0.9560546875,\n",
       " 0.9609375,\n",
       " 0.96875,\n",
       " 0.9423828125,\n",
       " 0.94921875,\n",
       " 0.9580078125,\n",
       " 0.9599609375,\n",
       " 0.951171875,\n",
       " 0.96484375,\n",
       " 0.96484375,\n",
       " 0.943359375,\n",
       " 0.943359375,\n",
       " 0.9462890625,\n",
       " 0.9521484375,\n",
       " 0.9501953125,\n",
       " 0.9619140625,\n",
       " 0.955078125,\n",
       " 0.943359375,\n",
       " 0.9609375,\n",
       " 0.9521484375,\n",
       " 0.95703125,\n",
       " 0.9619140625,\n",
       " 0.96484375,\n",
       " 0.962890625,\n",
       " 0.9609375,\n",
       " 0.9580078125,\n",
       " 0.9541015625,\n",
       " 0.9716796875,\n",
       " 0.9560546875,\n",
       " 0.96875,\n",
       " 0.9560546875,\n",
       " 0.9580078125,\n",
       " 0.9599609375,\n",
       " 0.9609375,\n",
       " 0.9619140625,\n",
       " 0.9599609375,\n",
       " 0.9521484375,\n",
       " 0.96875,\n",
       " 0.95703125,\n",
       " 0.9404296875,\n",
       " 0.9638671875,\n",
       " 0.9580078125,\n",
       " 0.953125,\n",
       " 0.9619140625,\n",
       " 0.9580078125,\n",
       " 0.958984375,\n",
       " 0.9619140625,\n",
       " 0.978515625,\n",
       " 0.9658203125,\n",
       " 0.962890625,\n",
       " 0.966796875,\n",
       " 0.9521484375,\n",
       " 0.9599609375,\n",
       " 0.970703125,\n",
       " 0.966796875,\n",
       " 0.9619140625,\n",
       " 0.970703125,\n",
       " 0.96875,\n",
       " 0.9677734375,\n",
       " 0.9697265625,\n",
       " 0.96875,\n",
       " 0.9775390625,\n",
       " 0.962890625,\n",
       " 0.958984375,\n",
       " 0.9775390625,\n",
       " 0.978515625,\n",
       " 0.962890625,\n",
       " 0.953125,\n",
       " 0.970703125,\n",
       " 0.9638671875,\n",
       " 0.9609375,\n",
       " 0.970703125,\n",
       " 0.9677734375,\n",
       " 0.9560546875,\n",
       " 0.9716796875,\n",
       " 0.962890625,\n",
       " 0.9697265625,\n",
       " 0.9736328125,\n",
       " 0.9521484375,\n",
       " 0.9716796875,\n",
       " 0.955078125,\n",
       " 0.9453125,\n",
       " 0.970703125,\n",
       " 0.962890625,\n",
       " 0.9609375,\n",
       " 0.96875,\n",
       " 0.9638671875,\n",
       " 0.9580078125,\n",
       " 0.9560546875,\n",
       " 0.9619140625,\n",
       " 0.953125,\n",
       " 0.958984375,\n",
       " 0.9619140625,\n",
       " 0.9755859375,\n",
       " 0.9638671875,\n",
       " 0.9677734375,\n",
       " 0.9560546875,\n",
       " 0.95703125,\n",
       " 0.9609375,\n",
       " 0.9677734375,\n",
       " 0.966796875,\n",
       " 0.9609375,\n",
       " 0.9609375,\n",
       " 0.9658203125,\n",
       " 0.9736328125,\n",
       " 0.9619140625,\n",
       " 0.9677734375,\n",
       " 0.9677734375,\n",
       " 0.97265625,\n",
       " 0.9677734375,\n",
       " 0.962890625,\n",
       " 0.96484375,\n",
       " 0.9560546875,\n",
       " 0.9541015625,\n",
       " 0.9697265625,\n",
       " 0.96484375,\n",
       " 0.9619140625,\n",
       " 0.9755859375,\n",
       " 0.9580078125,\n",
       " 0.9677734375,\n",
       " 0.96484375,\n",
       " 0.9697265625,\n",
       " 0.9599609375,\n",
       " 0.9638671875,\n",
       " 0.9736328125,\n",
       " 0.9619140625,\n",
       " 0.96484375,\n",
       " 0.96484375,\n",
       " 0.962890625,\n",
       " 0.9609375,\n",
       " 0.96484375,\n",
       " 0.96484375,\n",
       " 0.966796875,\n",
       " 0.9638671875,\n",
       " 0.9658203125,\n",
       " 0.962890625,\n",
       " 0.9658203125,\n",
       " 0.9736328125,\n",
       " 0.9619140625,\n",
       " 0.970703125,\n",
       " 0.9658203125,\n",
       " 0.9677734375,\n",
       " 0.9716796875,\n",
       " 0.96875,\n",
       " 0.962890625,\n",
       " 0.96875,\n",
       " 0.9677734375,\n",
       " 0.982421875,\n",
       " 0.96875,\n",
       " 0.9736328125,\n",
       " 0.9697265625,\n",
       " 0.962890625,\n",
       " 0.9609375,\n",
       " 0.97265625,\n",
       " 0.9560546875,\n",
       " 0.95703125,\n",
       " 0.9658203125,\n",
       " 0.9619140625,\n",
       " 0.9716796875,\n",
       " 0.9658203125,\n",
       " 0.96875,\n",
       " 0.9677734375,\n",
       " 0.9697265625,\n",
       " 0.9619140625,\n",
       " 0.9697265625,\n",
       " 0.9580078125,\n",
       " 0.96875,\n",
       " 0.9560546875,\n",
       " 0.9755859375,\n",
       " 0.974609375,\n",
       " 0.9677734375,\n",
       " 0.9716796875,\n",
       " 0.970703125,\n",
       " 0.9697265625,\n",
       " 0.9638671875,\n",
       " 0.96484375,\n",
       " 0.962890625,\n",
       " 0.962890625,\n",
       " 0.9638671875,\n",
       " 0.9609375,\n",
       " 0.943359375,\n",
       " 0.9580078125,\n",
       " 0.97265625,\n",
       " 0.953125,\n",
       " 0.953125,\n",
       " 0.9501953125,\n",
       " 0.9619140625,\n",
       " 0.9560546875,\n",
       " 0.9599609375,\n",
       " 0.951171875,\n",
       " 0.9677734375,\n",
       " 0.974609375,\n",
       " 0.9619140625,\n",
       " 0.974609375,\n",
       " 0.955078125,\n",
       " 0.953125,\n",
       " 0.9609375,\n",
       " 0.9423828125,\n",
       " 0.9755859375,\n",
       " 0.9736328125,\n",
       " 0.94921875,\n",
       " 0.955078125,\n",
       " 0.9609375,\n",
       " 0.962890625,\n",
       " 0.95703125,\n",
       " 0.98046875,\n",
       " 0.966796875,\n",
       " 0.9658203125,\n",
       " 0.97265625,\n",
       " 0.970703125,\n",
       " 0.95703125,\n",
       " 0.966796875,\n",
       " 0.978515625,\n",
       " 0.9638671875,\n",
       " 0.9638671875,\n",
       " 0.9716796875,\n",
       " 0.955078125,\n",
       " 0.9677734375,\n",
       " 0.9599609375,\n",
       " 0.951171875,\n",
       " 0.935546875,\n",
       " 0.962890625,\n",
       " 0.9599609375,\n",
       " 0.9794921875,\n",
       " 0.970703125,\n",
       " 0.9658203125,\n",
       " 0.9619140625,\n",
       " 0.98046875,\n",
       " 0.9765625,\n",
       " 0.986328125,\n",
       " 0.98046875,\n",
       " 0.98828125]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Explore_DoubleWell.acceptance_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2obaVPiOem-w"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Explore_DoubleWell",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "'SNF'",
   "language": "python",
   "name": "snf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
