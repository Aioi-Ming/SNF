{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca4e4366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fdf3d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4570ff8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7696fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#The model of the Decoder\n",
    "class GenerativeModel(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=50):\n",
    "        super(GenerativeModel, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.net = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(latent_dim, 1024),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(1024, 1024),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(1024, 784),\n",
    "                    torch.nn.Sigmoid()\n",
    "                    )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def sample(self, M, N=None):\n",
    "        device = next(self.parameters()).device\n",
    "        if N is None:\n",
    "            x = torch.randn(M, self.latent_dim).to(device)\n",
    "        else:\n",
    "            x = torch.randn(M, N, self.latent_dim).to(device)\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def conditional_log_likelihood(self, x, y):\n",
    "        recon_x = torch.clamp(self.forward(x), 1e-6, 1.-1e-6)\n",
    "        return torch.log(recon_x) * y + torch.log(1 - recon_x) * (1 - y)\n",
    "        \n",
    "class SimpleVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=50):\n",
    "        super(SimpleVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.G = GenerativeModel(latent_dim)\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(784, 1024),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(1024, 1024),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(1024, latent_dim * 2)\n",
    "                    )\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        device = next(self.parameters()).device\n",
    "        M = x.shape[0]\n",
    "        N = y.shape[0]\n",
    "        dW = torch.zeros((M, N, 1)).to(device)\n",
    "        mean_std = self.encoder(y)\n",
    "        mean = mean_std[:, :self.latent_dim]\n",
    "        std = torch.abs(mean_std[:, self.latent_dim:]) + 1e-6\n",
    "        x1 = x * std + mean\n",
    "        dW = dW + (x**2).sum(axis=2, keepdims=True) / 2\n",
    "        dW = dW - (x1**2).sum(axis=2, keepdims=True) / 2\n",
    "        dW = dW + self.G.conditional_log_likelihood(x1, y).sum(axis=2, keepdims=True)\n",
    "        dW = dW + torch.log(std).sum(axis=1, keepdims=True)\n",
    "        return x1, dW\n",
    "\n",
    "    def log_likelihood(self, y, M):\n",
    "        device = next(self.parameters()).device\n",
    "        x0 = torch.randn(M, y.shape[0], self.latent_dim).to(device)\n",
    "        x, dW = self.forward(x0, y.view(-1, 784))\n",
    "        return torch.mean(dW, axis=0, keepdims=False)\n",
    "\n",
    "class LangevinVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=50, nsteps=30, stepsize=0.01):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.G = GenerativeModel(latent_dim)\n",
    "        self.nsteps = nsteps\n",
    "        stepsize_list = torch.FloatTensor([stepsize,] * nsteps)\n",
    "        lambda_list = (np.array(range(1,nsteps + 1))/nsteps).tolist()\n",
    "        lambda_list = torch.FloatTensor(lambda_list)\n",
    "        self.stepsize_para_list, self.lambda_para_list = self.stepsize_lambda_2_para(stepsize_list, lambda_list)\n",
    "        self.stepsize_para_list = nn.Parameter(torch.FloatTensor(self.stepsize_para_list), requires_grad=True)\n",
    "        self.lambda_para_list = nn.Parameter(torch.FloatTensor(self.lambda_para_list), requires_grad=True)\n",
    "        \n",
    "    def stepsize_lambda_2_para(self, stepsize_list, lambda_list):\n",
    "        stepsize_para_list = torch.clamp(torch.abs(stepsize_list), min=1e-6)\n",
    "        lambda_para_list = lambda_list\n",
    "        return stepsize_para_list, lambda_para_list\n",
    "    \n",
    "    def para_2_stepsize_lambda(self, stepsize_para_list, lambda_para_list):\n",
    "        stepsize_list = torch.abs(stepsize_para_list) + 1e-6\n",
    "        lambda_list = lambda_para_list\n",
    "        return stepsize_list, lambda_list\n",
    "\n",
    "    def energy_0(self, x, y):\n",
    "        return (x**2).sum(axis=2, keepdims=True) / 2\n",
    "\n",
    "    def force_0(self, x, y):\n",
    "        return -x\n",
    "    \n",
    "    def sample_energy_0(self, y, M):\n",
    "        device = next(self.parameters()).device\n",
    "        x = torch.randn(M, y.shape[0], self.latent_dim).to(device)\n",
    "        return x\n",
    "        \n",
    "    def energy_1(self, x, y):\n",
    "        return (x**2).sum(axis=2, keepdims=True) / 2 - self.G.conditional_log_likelihood(x, y).sum(axis=2, keepdims=True)\n",
    "\n",
    "    def force_1(self, x, y):\n",
    "        x0 = x.clone().detach().requires_grad_(True)\n",
    "        e = self.energy_1(x0, y)\n",
    "        return -torch.autograd.grad(e.sum(), x0, create_graph=True)[0]\n",
    "\n",
    "    def interpolated_energy(self, x, y, lambda_=1.):\n",
    "        return self.energy_0(x, y) * (1 - lambda_) + self.energy_1(x, y) * lambda_\n",
    "\n",
    "    def interpolated_force(self, x, y, lambda_=1.):\n",
    "        return self.force_0(x, y) * (1 - lambda_) + self.force_1(x, y) * lambda_\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        stepsize_list, lambda_list = self.para_2_stepsize_lambda(self.stepsize_para_list, self.lambda_para_list)\n",
    "        dW = self.energy_0(x, y)\n",
    "        for i in range(self.nsteps):\n",
    "            lambda_ = lambda_list[i]\n",
    "            stepsize = stepsize_list[i]\n",
    "            # forward step\n",
    "            x1 = x + stepsize * self.interpolated_force(x, lambda_) + torch.sqrt(2*stepsize) * torch.randn_like(x)\n",
    "            tmp_dW = self.interpolated_energy(x1, y, lambda_) - self.interpolated_energy(x, y, lambda_)\n",
    "            A = torch.exp(torch.clamp(-tmp_dW, - math.inf, 0.))\n",
    "            u = torch.rand_like(A)\n",
    "            acc = (u <= A).float()\n",
    "            x = (1 - acc) * x + acc * x1\n",
    "            dW += acc * tmp_dW\n",
    "        dW = dW - self.energy_1(x, y)\n",
    "        return x, dW\n",
    "\n",
    "    def log_likelihood(self, y, M):\n",
    "        x0 = self.sample_energy_0(y.view(-1, 784), M)\n",
    "        x, dW = self.forward(x0, y.view(-1, 784))\n",
    "        return torch.mean(dW, axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "class CouplingLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, mask, cond_dim=None, s_tanh_activation=True, smooth_activation=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        if cond_dim is not None:\n",
    "            total_input_dim = input_dim + cond_dim\n",
    "        else:\n",
    "            total_input_dim = input_dim\n",
    "\n",
    "        self.s_fc1 = nn.Linear(total_input_dim, hid_dim)\n",
    "        self.s_fc2 = nn.Linear(hid_dim, hid_dim)\n",
    "        self.s_fc3 = nn.Linear(hid_dim, input_dim)\n",
    "        self.t_fc1 = nn.Linear(total_input_dim, hid_dim)\n",
    "        self.t_fc2 = nn.Linear(hid_dim, hid_dim)\n",
    "        self.t_fc3 = nn.Linear(hid_dim, input_dim)\n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "        self.s_tanh_activation = s_tanh_activation\n",
    "        self.smooth_activation = smooth_activation\n",
    "\n",
    "    def forward(self, x, cond_x=None, mode='direct'):\n",
    "        x_m = x * self.mask\n",
    "        if cond_x is not None:\n",
    "            x_m = torch.cat([x_m, cond_x.expand(x_m.shape[0], -1, -1)], -1)\n",
    "        if self.smooth_activation:\n",
    "            if self.s_tanh_activation:\n",
    "                s_out = torch.tanh(self.s_fc3(F.elu(self.s_fc2(F.elu(self.s_fc1(x_m)))))) * (1-self.mask)\n",
    "            else:\n",
    "                s_out = self.s_fc3(F.elu(self.s_fc2(F.elu(self.s_fc1(x_m))))) * (1-self.mask)\n",
    "            t_out = self.t_fc3(F.elu(self.t_fc2(F.elu(self.t_fc1(x_m))))) * (1-self.mask)\n",
    "        else:\n",
    "            if self.s_tanh_activation:\n",
    "                s_out = torch.tanh(self.s_fc3(F.relu(self.s_fc2(F.relu(self.s_fc1(x_m)))))) * (1-self.mask)\n",
    "            else:\n",
    "                s_out = self.s_fc3(F.relu(self.s_fc2(F.relu(self.s_fc1(x_m))))) * (1-self.mask)\n",
    "            t_out = self.t_fc3(F.relu(self.t_fc2(F.relu(self.t_fc1(x_m))))) * (1-self.mask)\n",
    "        if mode == 'direct':\n",
    "            y = x * torch.exp(s_out) + t_out\n",
    "            log_det_jacobian = s_out.sum(-1, keepdim=True)\n",
    "        else:\n",
    "            y = (x - t_out) * torch.exp(-s_out)\n",
    "            log_det_jacobian = -s_out.sum(-1, keepdim=True)\n",
    "        return y, log_det_jacobian\n",
    "\n",
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim = 256, n_layers = 2, cond_dim = None, s_tanh_activation = True, smooth_activation=False):\n",
    "        super().__init__()\n",
    "        assert n_layers >= 2, 'num of coupling layers should be greater or equal to 2'\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        mask = (torch.arange(0, input_dim) % 2).float()\n",
    "        self.modules = []\n",
    "        self.modules.append(CouplingLayer(input_dim, hid_dim, mask, cond_dim, s_tanh_activation, smooth_activation))\n",
    "        for _ in range(n_layers - 2):\n",
    "            mask = 1 - mask\n",
    "            self.modules.append(CouplingLayer(input_dim, hid_dim, mask, cond_dim, s_tanh_activation, smooth_activation))\n",
    "        self.modules.append(CouplingLayer(input_dim, hid_dim, 1 - mask, cond_dim, s_tanh_activation, smooth_activation))\n",
    "        self.module_list = nn.ModuleList(self.modules)\n",
    "        \n",
    "    def forward(self, x, cond_x=None, mode='direct'):\n",
    "        \"\"\" Performs a forward or backward pass for flow modules.\n",
    "        Args:\n",
    "            x: a tuple of inputs and logdets\n",
    "            mode: to run direct computation or inverse\n",
    "        \"\"\"\n",
    "        logdets = torch.zeros(x.size(), device=x.device).sum(-1, keepdim=True)\n",
    "\n",
    "        assert mode in ['direct', 'inverse']\n",
    "        if mode == 'direct':\n",
    "            for module in self.module_list:\n",
    "                x, logdet = module(x, cond_x, mode)\n",
    "                logdets += logdet\n",
    "        else:\n",
    "            for module in reversed(self.module_list):\n",
    "                x, logdet = module(x, cond_x, mode)\n",
    "                logdets += logdet\n",
    "\n",
    "        return x, logdets\n",
    "\n",
    "    def log_probs(self, x, cond_x = None):\n",
    "        u, log_jacob = self(x, cond_x)\n",
    "        log_probs = (-0.5 * u.pow(2) - 0.5 * math.log(2 * math.pi)).sum(\n",
    "            -1, keepdim=True)\n",
    "        return (log_probs + log_jacob).sum(-1, keepdim=True)\n",
    "\n",
    "    def sample(self, num_samples, noise=None, cond_x=None):\n",
    "        if noise is None:\n",
    "            noise = torch.Tensor(num_samples, self.input_dim).normal_()\n",
    "        device = next(self.parameters()).device\n",
    "        noise = noise.to(device)\n",
    "        if cond_x is not None:\n",
    "            cond_x = cond_x.to(device)\n",
    "        samples = self.forward(noise, cond_x, mode='inverse')[0]\n",
    "        return samples\n",
    "    \n",
    "class RealNVPVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=50):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.G = GenerativeModel(latent_dim)\n",
    "        self.F = RealNVP(latent_dim, hid_dim=64, n_layers=6, cond_dim=784)\n",
    "\n",
    "    def energy_0(self, x, y):\n",
    "        return (x**2).sum(axis=2, keepdims=True) / 2\n",
    "    \n",
    "    def sample_energy_0(self, y, M):\n",
    "        device = next(self.parameters()).device\n",
    "        x = torch.randn(M, y.shape[0], self.latent_dim).to(device)\n",
    "        return x\n",
    "        \n",
    "    def energy_1(self, x, y):\n",
    "        return (x**2).sum(axis=2, keepdims=True) / 2 - self.G.conditional_log_likelihood(x, y).sum(axis=2, keepdims=True)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        dW = self.energy_0(x, y)\n",
    "        x, tmp_dW = self.F(x, y)\n",
    "        dW += tmp_dW\n",
    "        dW = dW - self.energy_1(x, y)\n",
    "        return x, dW\n",
    "\n",
    "    def log_likelihood(self, y, M):\n",
    "        x0 = self.sample_energy_0(y.view(-1, 784), M)\n",
    "        x, dW = self.forward(x0, y.view(-1, 784))\n",
    "        return torch.mean(dW, axis=0, keepdims=False)\n",
    "\n",
    "class RealNVPVAE_eval(nn.Module):\n",
    "\n",
    "    def __init__(self, G):\n",
    "        super().__init__()\n",
    "        latent_dim = G.latent_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.G = G\n",
    "        self.F = RealNVP(latent_dim, hid_dim=256, n_layers=12, cond_dim=784)\n",
    "\n",
    "    def energy_0(self, x, y):\n",
    "        return (x**2).sum(axis=2, keepdims=True) / 2\n",
    "    \n",
    "    def sample_energy_0(self, y, M):\n",
    "        device = next(self.parameters()).device\n",
    "        x = torch.randn(M, y.shape[0], self.latent_dim).to(device)\n",
    "        return x\n",
    "        \n",
    "    def energy_1(self, x, y):\n",
    "        return (x**2).sum(axis=2, keepdims=True) / 2 - self.G.conditional_log_likelihood(x, y).sum(axis=2, keepdims=True)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        dW = self.energy_0(x, y)\n",
    "        x, tmp_dW = self.F(x, y)\n",
    "        dW += tmp_dW\n",
    "        dW = dW - self.energy_1(x, y)\n",
    "        return x, dW\n",
    "\n",
    "    def log_likelihood(self, y, M):\n",
    "        x0 = self.sample_energy_0(y.view(-1, 784), M)\n",
    "        x, dW = self.forward(x0, y.view(-1, 784))\n",
    "        return torch.logsumexp(dW, axis=0, keepdims=False) - math.log(M)\n",
    "\n",
    "def ModelEval(G, sample_size, data_file):\n",
    "    start = time.process_time()\n",
    "    \n",
    "#    device = torch.device(\"cuda\")\n",
    "    latent_dim = 50\n",
    "    batch_size = 128\n",
    "    n_epochs = 40\n",
    "    log_interval = 10\n",
    "\n",
    "    if data_file == 'mnist_data':\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            datasets.MNIST(data_file, train=True, download=False,\n",
    "                           transform=transforms.ToTensor()),\n",
    "                            batch_size=batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            datasets.MNIST(data_file, train=False, transform=transforms.ToTensor()),\n",
    "            batch_size=batch_size, shuffle=False)\n",
    "    else:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            datasets.FashionMNIST(data_file, train=True, download=False,\n",
    "                           transform=transforms.ToTensor()),\n",
    "                            batch_size=batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            datasets.FashionMNIST(data_file, train=False, transform=transforms.ToTensor()),\n",
    "            batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "    flow = RealNVPVAE_eval(G).to(device)\n",
    "    optim = torch.optim.Adam(flow.F.parameters(), lr=1e-3)\n",
    "\n",
    "    M = 1\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(test_loader):\n",
    "            data = ((torch.rand_like(data) <= data) + 0.).float()\n",
    "            data = data.to(device)\n",
    "            loss = -flow.log_likelihood(data, M).mean()\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()*len(data)\n",
    "            optim.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(test_loader.dataset),\n",
    "                    100. * batch_idx / len(test_loader),\n",
    "                    loss.item()))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        M = sample_size\n",
    "        K = 10\n",
    "        for kk in range(K):\n",
    "            for batch_idx, (data, _) in enumerate(test_loader):\n",
    "                data = ((torch.rand_like(data) <= data) + 0.).float()\n",
    "                data = data.to(device)\n",
    "                loss = -flow.log_likelihood(data, M).mean()\n",
    "                test_loss += loss.item()*len(data)\n",
    "                if batch_idx % log_interval == 0:\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        kk, batch_idx * len(data), len(test_loader.dataset),\n",
    "                        100. * batch_idx / len(test_loader),\n",
    "                        loss.item()))\n",
    "        test_loss /= len(test_loader.dataset)*K\n",
    "    print('====> Test set NLL: {:.4f}'.format(test_loss))\n",
    "\n",
    "    return test_loss\n",
    "    \n",
    "\n",
    "class SNFVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=50, unit_num=3, nsteps=10, stepsize=0.1):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.unit_num = unit_num\n",
    "        self.G = GenerativeModel(latent_dim)\n",
    "        self.F_list = []\n",
    "        for _ in range(unit_num):\n",
    "            self.F_list.append(RealNVP(latent_dim, hid_dim=64, n_layers=2, cond_dim=784))\n",
    "        self.F_list = nn.ModuleList(self.F_list)\n",
    "        self.nsteps = nsteps\n",
    "        stepsize_list = torch.FloatTensor([stepsize,] * nsteps * unit_num)\n",
    "        lambda_list = (np.array(range(1,nsteps * unit_num + 1))/nsteps / unit_num).tolist()\n",
    "        lambda_list = torch.FloatTensor(lambda_list)\n",
    "        self.stepsize_para_list, self.lambda_para_list = self.stepsize_lambda_2_para(stepsize_list, lambda_list)\n",
    "        self.stepsize_para_list = nn.Parameter(torch.FloatTensor(self.stepsize_para_list), requires_grad=True)\n",
    "        self.lambda_para_list = nn.Parameter(torch.FloatTensor(self.lambda_para_list))\n",
    "        \n",
    "    def stepsize_lambda_2_para(self, stepsize_list, lambda_list):\n",
    "        stepsize_para_list = torch.clamp(torch.abs(stepsize_list), min=1e-6)\n",
    "        lambda_para_list = lambda_list\n",
    "        return stepsize_para_list, lambda_para_list\n",
    "    \n",
    "    def para_2_stepsize_lambda(self, stepsize_para_list, lambda_para_list):\n",
    "        stepsize_list = torch.abs(stepsize_para_list) + 1e-6\n",
    "        lambda_list = lambda_para_list\n",
    "        return stepsize_list, lambda_list\n",
    "\n",
    "    def energy_0(self, x, y):\n",
    "        return (x**2).sum(axis=2, keepdims=True) / 2\n",
    "\n",
    "    def force_0(self, x, y):\n",
    "        return -x\n",
    "    \n",
    "    def sample_energy_0(self, y, M):\n",
    "        device = next(self.parameters()).device\n",
    "        x = torch.randn(M, y.shape[0], self.latent_dim).to(device)\n",
    "        return x\n",
    "        \n",
    "    def energy_1(self, x, y):\n",
    "        return (x**2).sum(axis=2, keepdims=True) / 2 - self.G.conditional_log_likelihood(x, y).sum(axis=2, keepdims=True)\n",
    "\n",
    "    def force_1(self, x, y):\n",
    "        x0 = x.clone().detach().requires_grad_(True)\n",
    "        e = self.energy_1(x0, y)\n",
    "        return -torch.autograd.grad(e.sum(), x0, create_graph=True)[0]\n",
    "\n",
    "    def interpolated_energy(self, x, y, lambda_=1.):\n",
    "        return self.energy_0(x, y) * (1 - lambda_) + self.energy_1(x, y) * lambda_\n",
    "\n",
    "    def interpolated_force(self, x, y, lambda_=1.):\n",
    "        return self.force_0(x, y) * (1 - lambda_) + self.force_1(x, y) * lambda_\n",
    "\n",
    "    def forward(self, x, y, flow_disable=False):\n",
    "        stepsize_list, lambda_list = self.para_2_stepsize_lambda(self.stepsize_para_list, self.lambda_para_list)\n",
    "        dW = self.energy_0(x, y)\n",
    "        for i in range(self.nsteps * self.unit_num):\n",
    "            if i % self.nsteps == 0:\n",
    "                x, tmp_dW = self.F_list[int(i/self.nsteps)](x, y)\n",
    "                dW += tmp_dW                \n",
    "            if flow_disable:\n",
    "                continue\n",
    "            lambda_ = lambda_list[i]\n",
    "            stepsize = stepsize_list[i]\n",
    "            # forward step\n",
    "            x1 = x + stepsize * self.interpolated_force(x, lambda_) + torch.sqrt(2*stepsize) * torch.randn_like(x)\n",
    "            tmp_dW = self.interpolated_energy(x1, y, lambda_) - self.interpolated_energy(x, y, lambda_)\n",
    "            A = torch.exp(torch.clamp(-tmp_dW, - math.inf, 0.))\n",
    "            u = torch.rand_like(A)\n",
    "            acc = (u <= A).float()\n",
    "            x = (1 - acc) * x + acc * x1\n",
    "            dW += acc * tmp_dW\n",
    "        dW = dW - self.energy_1(x, y)\n",
    "        return x, dW\n",
    "\n",
    "    def log_likelihood(self, y, M, flow_disable=False):\n",
    "        x0 = self.sample_energy_0(y.view(-1, 784), M)\n",
    "        x, dW = self.forward(x0, y.view(-1, 784), flow_disable)\n",
    "        return torch.mean(dW, axis=0, keepdims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf82f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name, data_file, M):\n",
    "    start = time.process_time()\n",
    "    \n",
    "    latent_dim = 50\n",
    "    batch_size = 128\n",
    "    log_interval = 100\n",
    "\n",
    "    if data_file == 'mnist_data':\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            datasets.MNIST('mnist_data', train=True, download=True,\n",
    "                           transform=transforms.ToTensor()),\n",
    "                            batch_size=batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            datasets.MNIST('mnist_data', train=False, transform=transforms.ToTensor()),\n",
    "            batch_size=batch_size, shuffle=True)\n",
    "    else:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            datasets.FashionMNIST('fashionmnist_data', train=True, download=True,\n",
    "                           transform=transforms.ToTensor()),\n",
    "                            batch_size=batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            datasets.FashionMNIST('fashionmnist_data', train=False, transform=transforms.ToTensor()),\n",
    "            batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    if model_name in ['SimpleVAE','RealNVPVAE','LangevinVAE']:\n",
    "        n_epochs = 40\n",
    "        if model_name == 'SimpleVAE':\n",
    "            flow = SimpleVAE(latent_dim).to(device)\n",
    "        if model_name == 'RealNVPVAE':\n",
    "            flow = RealNVPVAE(latent_dim).to(device)\n",
    "        if model_name == 'LangevinVAE':\n",
    "            flow = LangevinVAE(latent_dim).to(device)\n",
    "        optim = torch.optim.Adam(flow.parameters(), lr=1e-3)\n",
    "        #perform training\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            train_loss = 0\n",
    "            for batch_idx, (data, _) in enumerate(train_loader):\n",
    "                data = ((torch.rand_like(data) <= data) + 0.).float()\n",
    "                data = data.to(device)\n",
    "                loss = -flow.log_likelihood(data, M).mean()\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                train_loss += loss.item() * len(data)\n",
    "                optim.step()\n",
    "                if batch_idx % log_interval == 0:\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader),\n",
    "                        loss.item() * len(data) / len(data)))\n",
    "\n",
    "            test_loss = 0\n",
    "            for i, (data, _) in enumerate(test_loader):\n",
    "                data = ((torch.rand_like(data) <= data) + 0.).float()\n",
    "                data = data.to(device)\n",
    "                loss = -flow.log_likelihood(data, M).sum()\n",
    "                test_loss += loss.item()\n",
    "\n",
    "            test_loss /= len(test_loader.dataset)\n",
    "            print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    else:\n",
    "        flow = SNFVAE(latent_dim, nsteps=10, stepsize=1e-2).to(device)\n",
    "        optim = torch.optim.Adam(flow.parameters(), lr=1e-3)\n",
    "        n_epochs = 20\n",
    "        flow_disable = True\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            train_loss = 0\n",
    "            for batch_idx, (data, _) in enumerate(train_loader):\n",
    "                data = ((torch.rand_like(data) <= data) + 0.).float()\n",
    "                data = data.to(device)\n",
    "                loss = -flow.log_likelihood(data, M, flow_disable).mean()\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                train_loss += loss.item()\n",
    "                optim.step()\n",
    "                if batch_idx % log_interval == 0:\n",
    "                    print(flow.stepsize_para_list.mean())\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader),\n",
    "                        loss.item()))\n",
    "\n",
    "            test_loss = 0\n",
    "            for i, (data, _) in enumerate(test_loader):\n",
    "                data = ((torch.rand_like(data) <= data) + 0.).float()\n",
    "                data = data.to(device)\n",
    "                loss = -flow.log_likelihood(data, M, flow_disable).sum()\n",
    "                test_loss += loss.item()\n",
    "\n",
    "            test_loss /= len(test_loader.dataset)\n",
    "            print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "        optim = torch.optim.Adam(flow.parameters(), lr=1e-3)\n",
    "        flow_disable = False\n",
    "        n_epochs = 20\n",
    "        flow_disable = True\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            train_loss = 0\n",
    "            for batch_idx, (data, _) in enumerate(train_loader):\n",
    "                data = ((torch.rand_like(data) <= data) + 0.).float()\n",
    "                data = data.to(device)\n",
    "                loss = -flow.log_likelihood(data, M, flow_disable).mean()\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                train_loss += loss.item()\n",
    "                optim.step()\n",
    "                if batch_idx % log_interval == 0:\n",
    "                    print(flow.stepsize_para_list.mean())\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch+20, batch_idx * len(data), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader),\n",
    "                        loss.item()))\n",
    "\n",
    "            test_loss = 0\n",
    "            for i, (data, _) in enumerate(test_loader):\n",
    "                data = ((torch.rand_like(data) <= data) + 0.).float()\n",
    "                data = data.to(device)\n",
    "                loss = -flow.log_likelihood(data, M, flow_disable).sum()\n",
    "                test_loss += loss.item()\n",
    "\n",
    "            test_loss /= len(test_loader.dataset)\n",
    "            print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "\n",
    "    #calculate the marginal log-likelihood\n",
    "    loss = ModelEval(flow.G, 2000, data_file)\n",
    "\n",
    "    print('Running time: %s Seconds'%(time.process_time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae4d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 707.688599\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 194.843781\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 182.564392\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 152.178024\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 142.946747\n",
      "====> Test set loss: 133.4840\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 128.465347\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 121.582664\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 126.926620\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 119.762177\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 117.932770\n",
      "====> Test set loss: 115.5037\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 114.289276\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 112.614677\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 114.955811\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 107.567581\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 106.825333\n",
      "====> Test set loss: 109.0093\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 113.582375\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 108.989716\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 109.493332\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 105.533340\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 106.777267\n",
      "====> Test set loss: 105.6903\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 104.008377\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 107.873390\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 103.635681\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 105.216995\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 102.668404\n",
      "====> Test set loss: 103.1383\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 102.536453\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 101.602707\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 107.209579\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 104.479279\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 101.692482\n",
      "====> Test set loss: 101.3561\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 106.034645\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 98.468582\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 97.965607\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 99.805634\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 102.369736\n",
      "====> Test set loss: 100.2215\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 101.380753\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 97.597839\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 98.513504\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 99.034119\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 99.847305\n",
      "====> Test set loss: 99.5749\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 99.108337\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 100.240036\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 102.522041\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 105.915840\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 99.944679\n",
      "====> Test set loss: 99.1296\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 98.292458\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 102.669395\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 99.454544\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 100.283279\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 97.010651\n",
      "====> Test set loss: 98.5278\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 101.817154\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 95.831795\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 98.867264\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 103.212524\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 95.117973\n",
      "====> Test set loss: 97.8420\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 95.560318\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 98.918549\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 100.220566\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 96.918610\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 97.172104\n",
      "====> Test set loss: 97.8229\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 99.520905\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 95.960587\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 102.786652\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 96.091385\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 96.608154\n",
      "====> Test set loss: 97.5141\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 102.099319\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 94.532043\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 96.676605\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 98.602325\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 95.171677\n",
      "====> Test set loss: 97.1131\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 97.310173\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 93.004715\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 96.626038\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 96.796082\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 92.022423\n",
      "====> Test set loss: 96.9372\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 91.419952\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 94.298126\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 97.275078\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 99.193703\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 97.152161\n",
      "====> Test set loss: 96.5814\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 89.144440\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 93.447968\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 93.420502\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 96.336914\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 94.345390\n",
      "====> Test set loss: 96.4854\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 96.348099\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 99.316940\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 97.597778\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 98.269943\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 92.534966\n",
      "====> Test set loss: 96.5284\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 97.001526\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 93.460037\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 93.864288\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 93.516548\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 94.399017\n",
      "====> Test set loss: 96.0605\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 96.112961\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 99.686539\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 96.375572\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 97.974899\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 95.581467\n",
      "====> Test set loss: 96.3002\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 91.951233\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 97.969246\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 94.068069\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 98.328217\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 95.347687\n",
      "====> Test set loss: 95.7522\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 97.532867\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 90.828438\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 92.413612\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 92.518456\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 97.177567\n",
      "====> Test set loss: 95.8586\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 91.630249\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 98.185913\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 94.831039\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 92.277634\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 96.537872\n",
      "====> Test set loss: 95.7680\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 91.623581\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 92.517273\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 92.123749\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 100.211769\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 93.982574\n",
      "====> Test set loss: 95.4854\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 91.091400\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 88.261673\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 95.821228\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 95.254211\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 93.323532\n",
      "====> Test set loss: 95.3246\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 94.903549\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 94.286942\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 93.505814\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 90.828979\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 93.040871\n",
      "====> Test set loss: 95.4639\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 94.265884\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 98.266792\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 95.959618\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 95.557945\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 93.434036\n",
      "====> Test set loss: 95.2820\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 93.203568\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 93.851501\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 97.871689\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 99.857025\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 104.381836\n",
      "====> Test set loss: 98.6220\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 97.394188\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 101.312988\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 102.316833\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 96.582947\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 105.888611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 99.6384\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 101.222366\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 110.607742\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 97.817032\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 108.519943\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 123.258980\n",
      "====> Test set loss: 115.6982\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 116.858063\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 114.980148\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 127.837280\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 115.399307\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 120.874908\n",
      "====> Test set loss: 123.9392\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 121.468246\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 119.479630\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 129.505539\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 122.171082\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 117.821899\n",
      "====> Test set loss: 120.9274\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 118.738029\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 114.597366\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 123.179596\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 127.466339\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 123.796204\n",
      "====> Test set loss: 135.7423\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 133.860199\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 124.628784\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 134.038010\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 132.770767\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 126.640770\n",
      "====> Test set loss: 123.3449\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 116.864555\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 125.683533\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 124.739822\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 131.271515\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 126.111366\n",
      "====> Test set loss: 134.8622\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 134.236176\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 123.810242\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 127.072418\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 130.566559\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 127.385254\n",
      "====> Test set loss: 130.7790\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 132.104767\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 137.684998\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 138.570190\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 131.143005\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 117.292519\n",
      "====> Test set loss: 125.6630\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 128.443375\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 127.916054\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 130.500183\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 127.677124\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 121.063675\n",
      "====> Test set loss: 124.9299\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 126.544113\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 123.788383\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 123.360458\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 125.727692\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 128.256714\n",
      "====> Test set loss: 124.9883\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 119.657608\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 137.657272\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 129.594086\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 134.353088\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 141.746918\n",
      "====> Test set loss: 131.5430\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 326.403870\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 160.164215\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 140.801453\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: 135.015564\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 125.064285\n",
      "Train Epoch: 1 [6400/10000 (63%)]\tLoss: 124.064514\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 125.093613\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: 131.056610\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 117.942474\n",
      "Train Epoch: 2 [1280/10000 (13%)]\tLoss: 122.404793\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 118.831680\n",
      "Train Epoch: 2 [3840/10000 (38%)]\tLoss: 120.961639\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 117.226685\n",
      "Train Epoch: 2 [6400/10000 (63%)]\tLoss: 115.358276\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 116.196434\n",
      "Train Epoch: 2 [8960/10000 (89%)]\tLoss: 125.553665\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 115.515152\n",
      "Train Epoch: 3 [1280/10000 (13%)]\tLoss: 116.818588\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 113.877037\n",
      "Train Epoch: 3 [3840/10000 (38%)]\tLoss: 118.165512\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 115.199242\n",
      "Train Epoch: 3 [6400/10000 (63%)]\tLoss: 113.451996\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: 117.689041\n",
      "Train Epoch: 3 [8960/10000 (89%)]\tLoss: 123.554497\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: 114.027390\n",
      "Train Epoch: 4 [1280/10000 (13%)]\tLoss: 115.202873\n",
      "Train Epoch: 4 [2560/10000 (25%)]\tLoss: 112.975861\n",
      "Train Epoch: 4 [3840/10000 (38%)]\tLoss: 118.603455\n",
      "Train Epoch: 4 [5120/10000 (51%)]\tLoss: 113.316429\n",
      "Train Epoch: 4 [6400/10000 (63%)]\tLoss: 112.688957\n",
      "Train Epoch: 4 [7680/10000 (76%)]\tLoss: 113.159142\n",
      "Train Epoch: 4 [8960/10000 (89%)]\tLoss: 121.484924\n",
      "Train Epoch: 5 [0/10000 (0%)]\tLoss: 109.823212\n",
      "Train Epoch: 5 [1280/10000 (13%)]\tLoss: 112.876419\n",
      "Train Epoch: 5 [2560/10000 (25%)]\tLoss: 111.498871\n",
      "Train Epoch: 5 [3840/10000 (38%)]\tLoss: 116.820366\n",
      "Train Epoch: 5 [5120/10000 (51%)]\tLoss: 111.601952\n",
      "Train Epoch: 5 [6400/10000 (63%)]\tLoss: 112.302063\n",
      "Train Epoch: 5 [7680/10000 (76%)]\tLoss: 113.334671\n",
      "Train Epoch: 5 [8960/10000 (89%)]\tLoss: 117.649780\n",
      "Train Epoch: 6 [0/10000 (0%)]\tLoss: 110.753899\n",
      "Train Epoch: 6 [1280/10000 (13%)]\tLoss: 112.438301\n",
      "Train Epoch: 6 [2560/10000 (25%)]\tLoss: 111.144806\n",
      "Train Epoch: 6 [3840/10000 (38%)]\tLoss: 116.519867\n",
      "Train Epoch: 6 [5120/10000 (51%)]\tLoss: 112.277412\n",
      "Train Epoch: 6 [6400/10000 (63%)]\tLoss: 111.601654\n",
      "Train Epoch: 6 [7680/10000 (76%)]\tLoss: 112.875397\n",
      "Train Epoch: 6 [8960/10000 (89%)]\tLoss: 120.611374\n",
      "Train Epoch: 7 [0/10000 (0%)]\tLoss: 109.277298\n",
      "Train Epoch: 7 [1280/10000 (13%)]\tLoss: 111.774109\n",
      "Train Epoch: 7 [2560/10000 (25%)]\tLoss: 110.367233\n",
      "Train Epoch: 7 [3840/10000 (38%)]\tLoss: 116.258362\n",
      "Train Epoch: 7 [5120/10000 (51%)]\tLoss: 111.368301\n",
      "Train Epoch: 7 [6400/10000 (63%)]\tLoss: 110.339600\n",
      "Train Epoch: 7 [7680/10000 (76%)]\tLoss: 113.402618\n",
      "Train Epoch: 7 [8960/10000 (89%)]\tLoss: 117.898270\n",
      "Train Epoch: 8 [0/10000 (0%)]\tLoss: 109.317261\n",
      "Train Epoch: 8 [1280/10000 (13%)]\tLoss: 112.634544\n",
      "Train Epoch: 8 [2560/10000 (25%)]\tLoss: 111.549828\n",
      "Train Epoch: 8 [3840/10000 (38%)]\tLoss: 114.848145\n",
      "Train Epoch: 8 [5120/10000 (51%)]\tLoss: 111.280060\n",
      "Train Epoch: 8 [6400/10000 (63%)]\tLoss: 110.501740\n",
      "Train Epoch: 8 [7680/10000 (76%)]\tLoss: 112.872910\n",
      "Train Epoch: 8 [8960/10000 (89%)]\tLoss: 118.512680\n",
      "Train Epoch: 9 [0/10000 (0%)]\tLoss: 107.783264\n",
      "Train Epoch: 9 [1280/10000 (13%)]\tLoss: 113.842270\n",
      "Train Epoch: 9 [2560/10000 (25%)]\tLoss: 111.730164\n",
      "Train Epoch: 9 [3840/10000 (38%)]\tLoss: 115.425415\n",
      "Train Epoch: 9 [5120/10000 (51%)]\tLoss: 112.025406\n",
      "Train Epoch: 9 [6400/10000 (63%)]\tLoss: 111.658813\n",
      "Train Epoch: 9 [7680/10000 (76%)]\tLoss: 113.581955\n",
      "Train Epoch: 9 [8960/10000 (89%)]\tLoss: 117.214684\n",
      "Train Epoch: 10 [0/10000 (0%)]\tLoss: 108.042725\n",
      "Train Epoch: 10 [1280/10000 (13%)]\tLoss: 111.354416\n",
      "Train Epoch: 10 [2560/10000 (25%)]\tLoss: 110.100319\n",
      "Train Epoch: 10 [3840/10000 (38%)]\tLoss: 114.739204\n",
      "Train Epoch: 10 [5120/10000 (51%)]\tLoss: 111.269806\n",
      "Train Epoch: 10 [6400/10000 (63%)]\tLoss: 109.461777\n",
      "Train Epoch: 10 [7680/10000 (76%)]\tLoss: 112.096886\n",
      "Train Epoch: 10 [8960/10000 (89%)]\tLoss: 116.526268\n",
      "Train Epoch: 11 [0/10000 (0%)]\tLoss: 108.106186\n",
      "Train Epoch: 11 [1280/10000 (13%)]\tLoss: 111.104332\n",
      "Train Epoch: 11 [2560/10000 (25%)]\tLoss: 110.290123\n",
      "Train Epoch: 11 [3840/10000 (38%)]\tLoss: 115.308983\n",
      "Train Epoch: 11 [5120/10000 (51%)]\tLoss: 110.500359\n",
      "Train Epoch: 11 [6400/10000 (63%)]\tLoss: 109.232834\n",
      "Train Epoch: 11 [7680/10000 (76%)]\tLoss: 111.349884\n",
      "Train Epoch: 11 [8960/10000 (89%)]\tLoss: 117.970337\n",
      "Train Epoch: 12 [0/10000 (0%)]\tLoss: 108.752060\n",
      "Train Epoch: 12 [1280/10000 (13%)]\tLoss: 111.746811\n",
      "Train Epoch: 12 [2560/10000 (25%)]\tLoss: 110.646881\n",
      "Train Epoch: 12 [3840/10000 (38%)]\tLoss: 115.074677\n",
      "Train Epoch: 12 [5120/10000 (51%)]\tLoss: 110.561722\n",
      "Train Epoch: 12 [6400/10000 (63%)]\tLoss: 110.192131\n",
      "Train Epoch: 12 [7680/10000 (76%)]\tLoss: 111.789162\n",
      "Train Epoch: 12 [8960/10000 (89%)]\tLoss: 116.494705\n",
      "Train Epoch: 13 [0/10000 (0%)]\tLoss: 106.615753\n",
      "Train Epoch: 13 [1280/10000 (13%)]\tLoss: 112.237320\n",
      "Train Epoch: 13 [2560/10000 (25%)]\tLoss: 110.456108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [3840/10000 (38%)]\tLoss: 114.133362\n",
      "Train Epoch: 13 [5120/10000 (51%)]\tLoss: 110.853615\n",
      "Train Epoch: 13 [6400/10000 (63%)]\tLoss: 108.786728\n",
      "Train Epoch: 13 [7680/10000 (76%)]\tLoss: 111.702652\n",
      "Train Epoch: 13 [8960/10000 (89%)]\tLoss: 117.086601\n",
      "Train Epoch: 14 [0/10000 (0%)]\tLoss: 107.282875\n",
      "Train Epoch: 14 [1280/10000 (13%)]\tLoss: 111.558678\n",
      "Train Epoch: 14 [2560/10000 (25%)]\tLoss: 109.686966\n",
      "Train Epoch: 14 [3840/10000 (38%)]\tLoss: 114.191223\n",
      "Train Epoch: 14 [5120/10000 (51%)]\tLoss: 110.314713\n",
      "Train Epoch: 14 [6400/10000 (63%)]\tLoss: 109.374649\n",
      "Train Epoch: 14 [7680/10000 (76%)]\tLoss: 109.551468\n",
      "Train Epoch: 14 [8960/10000 (89%)]\tLoss: 116.866516\n",
      "Train Epoch: 15 [0/10000 (0%)]\tLoss: 109.324455\n",
      "Train Epoch: 15 [1280/10000 (13%)]\tLoss: 110.114143\n",
      "Train Epoch: 15 [2560/10000 (25%)]\tLoss: 110.043915\n",
      "Train Epoch: 15 [3840/10000 (38%)]\tLoss: 113.841743\n",
      "Train Epoch: 15 [5120/10000 (51%)]\tLoss: 110.264572\n",
      "Train Epoch: 15 [6400/10000 (63%)]\tLoss: 110.062042\n",
      "Train Epoch: 15 [7680/10000 (76%)]\tLoss: 110.949898\n",
      "Train Epoch: 15 [8960/10000 (89%)]\tLoss: 115.793076\n",
      "Train Epoch: 16 [0/10000 (0%)]\tLoss: 106.909012\n",
      "Train Epoch: 16 [1280/10000 (13%)]\tLoss: 110.905624\n",
      "Train Epoch: 16 [2560/10000 (25%)]\tLoss: 109.088852\n",
      "Train Epoch: 16 [3840/10000 (38%)]\tLoss: 114.829895\n",
      "Train Epoch: 16 [5120/10000 (51%)]\tLoss: 110.719765\n",
      "Train Epoch: 16 [6400/10000 (63%)]\tLoss: 109.167786\n",
      "Train Epoch: 16 [7680/10000 (76%)]\tLoss: 110.931572\n",
      "Train Epoch: 16 [8960/10000 (89%)]\tLoss: 118.064011\n",
      "Train Epoch: 17 [0/10000 (0%)]\tLoss: 107.030945\n",
      "Train Epoch: 17 [1280/10000 (13%)]\tLoss: 111.971588\n",
      "Train Epoch: 17 [2560/10000 (25%)]\tLoss: 110.143066\n",
      "Train Epoch: 17 [3840/10000 (38%)]\tLoss: 114.179214\n",
      "Train Epoch: 17 [5120/10000 (51%)]\tLoss: 109.473801\n",
      "Train Epoch: 17 [6400/10000 (63%)]\tLoss: 107.626572\n",
      "Train Epoch: 17 [7680/10000 (76%)]\tLoss: 110.722305\n",
      "Train Epoch: 17 [8960/10000 (89%)]\tLoss: 115.913666\n",
      "Train Epoch: 18 [0/10000 (0%)]\tLoss: 107.122383\n",
      "Train Epoch: 18 [1280/10000 (13%)]\tLoss: 110.374641\n",
      "Train Epoch: 18 [2560/10000 (25%)]\tLoss: 109.990463\n",
      "Train Epoch: 18 [3840/10000 (38%)]\tLoss: 114.285728\n",
      "Train Epoch: 18 [5120/10000 (51%)]\tLoss: 109.398834\n",
      "Train Epoch: 18 [6400/10000 (63%)]\tLoss: 107.729416\n",
      "Train Epoch: 18 [7680/10000 (76%)]\tLoss: 109.956970\n",
      "Train Epoch: 18 [8960/10000 (89%)]\tLoss: 115.813797\n",
      "Train Epoch: 19 [0/10000 (0%)]\tLoss: 106.727051\n",
      "Train Epoch: 19 [1280/10000 (13%)]\tLoss: 109.455826\n",
      "Train Epoch: 19 [2560/10000 (25%)]\tLoss: 109.758377\n",
      "Train Epoch: 19 [3840/10000 (38%)]\tLoss: 113.236580\n",
      "Train Epoch: 19 [5120/10000 (51%)]\tLoss: 108.334320\n",
      "Train Epoch: 19 [6400/10000 (63%)]\tLoss: 108.585373\n",
      "Train Epoch: 19 [7680/10000 (76%)]\tLoss: 109.679108\n",
      "Train Epoch: 19 [8960/10000 (89%)]\tLoss: 115.139893\n",
      "Train Epoch: 20 [0/10000 (0%)]\tLoss: 106.323662\n",
      "Train Epoch: 20 [1280/10000 (13%)]\tLoss: 110.073059\n",
      "Train Epoch: 20 [2560/10000 (25%)]\tLoss: 108.904266\n",
      "Train Epoch: 20 [3840/10000 (38%)]\tLoss: 113.095718\n",
      "Train Epoch: 20 [5120/10000 (51%)]\tLoss: 109.593109\n",
      "Train Epoch: 20 [6400/10000 (63%)]\tLoss: 107.193451\n",
      "Train Epoch: 20 [7680/10000 (76%)]\tLoss: 111.380020\n",
      "Train Epoch: 20 [8960/10000 (89%)]\tLoss: 116.284454\n",
      "Train Epoch: 21 [0/10000 (0%)]\tLoss: 107.785576\n",
      "Train Epoch: 21 [1280/10000 (13%)]\tLoss: 111.196899\n",
      "Train Epoch: 21 [2560/10000 (25%)]\tLoss: 108.985306\n",
      "Train Epoch: 21 [3840/10000 (38%)]\tLoss: 114.008087\n",
      "Train Epoch: 21 [5120/10000 (51%)]\tLoss: 108.792313\n",
      "Train Epoch: 21 [6400/10000 (63%)]\tLoss: 109.266998\n",
      "Train Epoch: 21 [7680/10000 (76%)]\tLoss: 109.859024\n",
      "Train Epoch: 21 [8960/10000 (89%)]\tLoss: 116.040176\n",
      "Train Epoch: 22 [0/10000 (0%)]\tLoss: 106.229584\n",
      "Train Epoch: 22 [1280/10000 (13%)]\tLoss: 110.130127\n",
      "Train Epoch: 22 [2560/10000 (25%)]\tLoss: 107.787415\n",
      "Train Epoch: 22 [3840/10000 (38%)]\tLoss: 112.924850\n",
      "Train Epoch: 22 [5120/10000 (51%)]\tLoss: 109.467041\n",
      "Train Epoch: 22 [6400/10000 (63%)]\tLoss: 108.819130\n",
      "Train Epoch: 22 [7680/10000 (76%)]\tLoss: 111.723587\n",
      "Train Epoch: 22 [8960/10000 (89%)]\tLoss: 115.939003\n",
      "Train Epoch: 23 [0/10000 (0%)]\tLoss: 106.980682\n",
      "Train Epoch: 23 [1280/10000 (13%)]\tLoss: 111.462044\n",
      "Train Epoch: 23 [2560/10000 (25%)]\tLoss: 108.040688\n",
      "Train Epoch: 23 [3840/10000 (38%)]\tLoss: 113.787926\n",
      "Train Epoch: 23 [5120/10000 (51%)]\tLoss: 109.803391\n",
      "Train Epoch: 23 [6400/10000 (63%)]\tLoss: 108.463936\n",
      "Train Epoch: 23 [7680/10000 (76%)]\tLoss: 110.577454\n",
      "Train Epoch: 23 [8960/10000 (89%)]\tLoss: 115.976570\n",
      "Train Epoch: 24 [0/10000 (0%)]\tLoss: 105.371658\n",
      "Train Epoch: 24 [1280/10000 (13%)]\tLoss: 110.048264\n",
      "Train Epoch: 24 [2560/10000 (25%)]\tLoss: 107.909851\n",
      "Train Epoch: 24 [3840/10000 (38%)]\tLoss: 113.616844\n",
      "Train Epoch: 24 [5120/10000 (51%)]\tLoss: 109.475891\n",
      "Train Epoch: 24 [6400/10000 (63%)]\tLoss: 107.822098\n",
      "Train Epoch: 24 [7680/10000 (76%)]\tLoss: 110.979919\n",
      "Train Epoch: 24 [8960/10000 (89%)]\tLoss: 115.094833\n",
      "Train Epoch: 25 [0/10000 (0%)]\tLoss: 104.476883\n",
      "Train Epoch: 25 [1280/10000 (13%)]\tLoss: 109.509239\n",
      "Train Epoch: 25 [2560/10000 (25%)]\tLoss: 107.531464\n",
      "Train Epoch: 25 [3840/10000 (38%)]\tLoss: 112.799500\n",
      "Train Epoch: 25 [5120/10000 (51%)]\tLoss: 108.865273\n",
      "Train Epoch: 25 [6400/10000 (63%)]\tLoss: 106.819763\n",
      "Train Epoch: 25 [7680/10000 (76%)]\tLoss: 110.499847\n",
      "Train Epoch: 25 [8960/10000 (89%)]\tLoss: 115.439537\n",
      "Train Epoch: 26 [0/10000 (0%)]\tLoss: 105.848679\n",
      "Train Epoch: 26 [1280/10000 (13%)]\tLoss: 110.355392\n",
      "Train Epoch: 26 [2560/10000 (25%)]\tLoss: 107.995255\n",
      "Train Epoch: 26 [3840/10000 (38%)]\tLoss: 113.139618\n",
      "Train Epoch: 26 [5120/10000 (51%)]\tLoss: 109.771416\n",
      "Train Epoch: 26 [6400/10000 (63%)]\tLoss: 109.006561\n",
      "Train Epoch: 26 [7680/10000 (76%)]\tLoss: 108.989738\n",
      "Train Epoch: 26 [8960/10000 (89%)]\tLoss: 116.657181\n",
      "Train Epoch: 27 [0/10000 (0%)]\tLoss: 106.648567\n",
      "Train Epoch: 27 [1280/10000 (13%)]\tLoss: 108.859741\n",
      "Train Epoch: 27 [2560/10000 (25%)]\tLoss: 107.812813\n",
      "Train Epoch: 27 [3840/10000 (38%)]\tLoss: 111.981422\n",
      "Train Epoch: 27 [5120/10000 (51%)]\tLoss: 109.208832\n",
      "Train Epoch: 27 [6400/10000 (63%)]\tLoss: 108.493385\n",
      "Train Epoch: 27 [7680/10000 (76%)]\tLoss: 109.548889\n",
      "Train Epoch: 27 [8960/10000 (89%)]\tLoss: 115.764069\n",
      "Train Epoch: 28 [0/10000 (0%)]\tLoss: 106.810020\n",
      "Train Epoch: 28 [1280/10000 (13%)]\tLoss: 108.925865\n",
      "Train Epoch: 28 [2560/10000 (25%)]\tLoss: 110.049217\n",
      "Train Epoch: 28 [3840/10000 (38%)]\tLoss: 112.998810\n",
      "Train Epoch: 28 [5120/10000 (51%)]\tLoss: 108.080780\n",
      "Train Epoch: 28 [6400/10000 (63%)]\tLoss: 108.879364\n",
      "Train Epoch: 28 [7680/10000 (76%)]\tLoss: 109.548721\n",
      "Train Epoch: 28 [8960/10000 (89%)]\tLoss: 114.493881\n",
      "Train Epoch: 29 [0/10000 (0%)]\tLoss: 107.057030\n",
      "Train Epoch: 29 [1280/10000 (13%)]\tLoss: 110.618958\n",
      "Train Epoch: 29 [2560/10000 (25%)]\tLoss: 108.110786\n",
      "Train Epoch: 29 [3840/10000 (38%)]\tLoss: 112.767212\n",
      "Train Epoch: 29 [5120/10000 (51%)]\tLoss: 109.109451\n",
      "Train Epoch: 29 [6400/10000 (63%)]\tLoss: 108.155304\n",
      "Train Epoch: 29 [7680/10000 (76%)]\tLoss: 109.543510\n",
      "Train Epoch: 29 [8960/10000 (89%)]\tLoss: 115.361725\n",
      "Train Epoch: 30 [0/10000 (0%)]\tLoss: 105.243210\n",
      "Train Epoch: 30 [1280/10000 (13%)]\tLoss: 108.829269\n",
      "Train Epoch: 30 [2560/10000 (25%)]\tLoss: 108.009270\n",
      "Train Epoch: 30 [3840/10000 (38%)]\tLoss: 112.438484\n",
      "Train Epoch: 30 [5120/10000 (51%)]\tLoss: 108.265778\n",
      "Train Epoch: 30 [6400/10000 (63%)]\tLoss: 108.908829\n",
      "Train Epoch: 30 [7680/10000 (76%)]\tLoss: 111.007492\n",
      "Train Epoch: 30 [8960/10000 (89%)]\tLoss: 114.688255\n",
      "Train Epoch: 31 [0/10000 (0%)]\tLoss: 106.004677\n",
      "Train Epoch: 31 [1280/10000 (13%)]\tLoss: 109.859451\n",
      "Train Epoch: 31 [2560/10000 (25%)]\tLoss: 108.048813\n",
      "Train Epoch: 31 [3840/10000 (38%)]\tLoss: 112.898315\n",
      "Train Epoch: 31 [5120/10000 (51%)]\tLoss: 109.112312\n",
      "Train Epoch: 31 [6400/10000 (63%)]\tLoss: 108.810959\n",
      "Train Epoch: 31 [7680/10000 (76%)]\tLoss: 109.216125\n",
      "Train Epoch: 31 [8960/10000 (89%)]\tLoss: 114.036003\n",
      "Train Epoch: 32 [0/10000 (0%)]\tLoss: 105.297897\n",
      "Train Epoch: 32 [1280/10000 (13%)]\tLoss: 109.577026\n",
      "Train Epoch: 32 [2560/10000 (25%)]\tLoss: 108.089516\n",
      "Train Epoch: 32 [3840/10000 (38%)]\tLoss: 112.091721\n",
      "Train Epoch: 32 [5120/10000 (51%)]\tLoss: 108.458923\n",
      "Train Epoch: 32 [6400/10000 (63%)]\tLoss: 109.036064\n",
      "Train Epoch: 32 [7680/10000 (76%)]\tLoss: 110.199303\n",
      "Train Epoch: 32 [8960/10000 (89%)]\tLoss: 116.373329\n",
      "Train Epoch: 33 [0/10000 (0%)]\tLoss: 106.523346\n",
      "Train Epoch: 33 [1280/10000 (13%)]\tLoss: 109.951584\n",
      "Train Epoch: 33 [2560/10000 (25%)]\tLoss: 108.067581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 33 [3840/10000 (38%)]\tLoss: 113.317375\n",
      "Train Epoch: 33 [5120/10000 (51%)]\tLoss: 109.429901\n",
      "Train Epoch: 33 [6400/10000 (63%)]\tLoss: 108.587051\n",
      "Train Epoch: 33 [7680/10000 (76%)]\tLoss: 109.746857\n",
      "Train Epoch: 33 [8960/10000 (89%)]\tLoss: 114.955750\n",
      "Train Epoch: 34 [0/10000 (0%)]\tLoss: 105.967278\n",
      "Train Epoch: 34 [1280/10000 (13%)]\tLoss: 109.335266\n",
      "Train Epoch: 34 [2560/10000 (25%)]\tLoss: 107.576324\n",
      "Train Epoch: 34 [3840/10000 (38%)]\tLoss: 113.098145\n",
      "Train Epoch: 34 [5120/10000 (51%)]\tLoss: 108.887650\n",
      "Train Epoch: 34 [6400/10000 (63%)]\tLoss: 107.566795\n",
      "Train Epoch: 34 [7680/10000 (76%)]\tLoss: 110.139771\n",
      "Train Epoch: 34 [8960/10000 (89%)]\tLoss: 115.447861\n",
      "Train Epoch: 35 [0/10000 (0%)]\tLoss: 105.525436\n",
      "Train Epoch: 35 [1280/10000 (13%)]\tLoss: 108.887665\n",
      "Train Epoch: 35 [2560/10000 (25%)]\tLoss: 108.284348\n",
      "Train Epoch: 35 [3840/10000 (38%)]\tLoss: 112.619598\n",
      "Train Epoch: 35 [5120/10000 (51%)]\tLoss: 108.709610\n",
      "Train Epoch: 35 [6400/10000 (63%)]\tLoss: 108.755875\n",
      "Train Epoch: 35 [7680/10000 (76%)]\tLoss: 109.920822\n",
      "Train Epoch: 35 [8960/10000 (89%)]\tLoss: 115.052292\n",
      "Train Epoch: 36 [0/10000 (0%)]\tLoss: 105.552444\n",
      "Train Epoch: 36 [1280/10000 (13%)]\tLoss: 109.782478\n",
      "Train Epoch: 36 [2560/10000 (25%)]\tLoss: 108.021881\n",
      "Train Epoch: 36 [3840/10000 (38%)]\tLoss: 113.598083\n",
      "Train Epoch: 36 [5120/10000 (51%)]\tLoss: 107.827637\n",
      "Train Epoch: 36 [6400/10000 (63%)]\tLoss: 108.471085\n",
      "Train Epoch: 36 [7680/10000 (76%)]\tLoss: 109.400803\n",
      "Train Epoch: 36 [8960/10000 (89%)]\tLoss: 114.615540\n",
      "Train Epoch: 37 [0/10000 (0%)]\tLoss: 105.732628\n",
      "Train Epoch: 37 [1280/10000 (13%)]\tLoss: 108.503113\n",
      "Train Epoch: 37 [2560/10000 (25%)]\tLoss: 108.295891\n",
      "Train Epoch: 37 [3840/10000 (38%)]\tLoss: 112.482681\n",
      "Train Epoch: 37 [5120/10000 (51%)]\tLoss: 109.243904\n",
      "Train Epoch: 37 [6400/10000 (63%)]\tLoss: 107.813354\n",
      "Train Epoch: 37 [7680/10000 (76%)]\tLoss: 109.284035\n",
      "Train Epoch: 37 [8960/10000 (89%)]\tLoss: 114.592163\n",
      "Train Epoch: 38 [0/10000 (0%)]\tLoss: 105.871384\n",
      "Train Epoch: 38 [1280/10000 (13%)]\tLoss: 109.434616\n",
      "Train Epoch: 38 [2560/10000 (25%)]\tLoss: 108.077301\n",
      "Train Epoch: 38 [3840/10000 (38%)]\tLoss: 112.283859\n",
      "Train Epoch: 38 [5120/10000 (51%)]\tLoss: 108.849045\n",
      "Train Epoch: 38 [6400/10000 (63%)]\tLoss: 108.140518\n",
      "Train Epoch: 38 [7680/10000 (76%)]\tLoss: 109.271690\n",
      "Train Epoch: 38 [8960/10000 (89%)]\tLoss: 113.316711\n",
      "Train Epoch: 39 [0/10000 (0%)]\tLoss: 104.923569\n",
      "Train Epoch: 39 [1280/10000 (13%)]\tLoss: 109.579323\n",
      "Train Epoch: 39 [2560/10000 (25%)]\tLoss: 108.607864\n",
      "Train Epoch: 39 [3840/10000 (38%)]\tLoss: 112.324448\n",
      "Train Epoch: 39 [5120/10000 (51%)]\tLoss: 107.822914\n",
      "Train Epoch: 39 [6400/10000 (63%)]\tLoss: 108.074387\n",
      "Train Epoch: 39 [7680/10000 (76%)]\tLoss: 108.841644\n",
      "Train Epoch: 39 [8960/10000 (89%)]\tLoss: 113.773811\n",
      "Train Epoch: 40 [0/10000 (0%)]\tLoss: 105.562134\n",
      "Train Epoch: 40 [1280/10000 (13%)]\tLoss: 110.012947\n",
      "Train Epoch: 40 [2560/10000 (25%)]\tLoss: 109.262581\n",
      "Train Epoch: 40 [3840/10000 (38%)]\tLoss: 112.044693\n",
      "Train Epoch: 40 [5120/10000 (51%)]\tLoss: 108.541328\n",
      "Train Epoch: 40 [6400/10000 (63%)]\tLoss: 107.889687\n",
      "Train Epoch: 40 [7680/10000 (76%)]\tLoss: 109.717438\n",
      "Train Epoch: 40 [8960/10000 (89%)]\tLoss: 114.228661\n",
      "Train Epoch: 0 [0/10000 (0%)]\tLoss: 99.406982\n",
      "Train Epoch: 0 [1280/10000 (13%)]\tLoss: 103.817253\n",
      "Train Epoch: 0 [2560/10000 (25%)]\tLoss: 103.214149\n",
      "Train Epoch: 0 [3840/10000 (38%)]\tLoss: 107.894150\n",
      "Train Epoch: 0 [5120/10000 (51%)]\tLoss: 104.527893\n",
      "Train Epoch: 0 [6400/10000 (63%)]\tLoss: 102.460121\n",
      "Train Epoch: 0 [7680/10000 (76%)]\tLoss: 104.755493\n",
      "Train Epoch: 0 [8960/10000 (89%)]\tLoss: 109.132805\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 100.381065\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 103.856949\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 103.314758\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: 108.393356\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 104.133316\n",
      "Train Epoch: 1 [6400/10000 (63%)]\tLoss: 103.657639\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 104.891846\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: 109.142395\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 100.806824\n",
      "Train Epoch: 2 [1280/10000 (13%)]\tLoss: 104.278992\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 103.281494\n",
      "Train Epoch: 2 [3840/10000 (38%)]\tLoss: 107.573135\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 103.526352\n",
      "Train Epoch: 2 [6400/10000 (63%)]\tLoss: 104.092903\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 104.525993\n",
      "Train Epoch: 2 [8960/10000 (89%)]\tLoss: 109.211517\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 100.848633\n",
      "Train Epoch: 3 [1280/10000 (13%)]\tLoss: 104.479370\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 103.281731\n"
     ]
    }
   ],
   "source": [
    "M = 5\n",
    "for model in ['SimpleVAE', 'RealNVPVAE', 'LangevinVAE', 'SNFVAE']:\n",
    "    for data_file in ['mnist_data', 'fashionmnist_data']:\n",
    "        train(model, data_file, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feec39f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'SNF'",
   "language": "python",
   "name": "snf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
